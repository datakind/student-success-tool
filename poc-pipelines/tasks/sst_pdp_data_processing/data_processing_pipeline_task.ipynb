{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5c5d41a-57e2-4b2c-a1fb-129131bef244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script prepares data for inference in the Student Success Tool (SST) pipeline.\n",
    "\n",
    "It reads validated course and cohort data from Delta Lake tables, creates a student-term \n",
    "dataset, applies target variable logic (currently using a workaround due to library \n",
    "limitations), and saves the processed dataset back to a Delta Lake table.  It's \n",
    "designed to run within a Databricks environment.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "\n",
    "import mlflow\n",
    "from databricks.connect import DatabricksSession\n",
    "from databricks.sdk.runtime import dbutils\n",
    "\n",
    "from student_success_tool.analysis import pdp\n",
    "from student_success_tool import configs\n",
    "\n",
    "# Disable mlflow autologging (due to Databricks issues during feature selection)\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.WARNING)  # Ignore Databricks logger\n",
    "\n",
    "# Attempt to create a Spark session\n",
    "try:\n",
    "    spark_session = DatabricksSession.builder.getOrCreate()\n",
    "except Exception:\n",
    "    logging.warning(\"Unable to create Spark session; are you in a Databricks runtime?\")\n",
    "    spark_session = None\n",
    "\n",
    "# Databricks workspace identifier\n",
    "DB_workspace = dbutils.widgets.get(\"DB_workspace\")\n",
    "\n",
    "# Input parameters from Databricks widgets\n",
    "institution_id = dbutils.widgets.get(\"institution_id\")\n",
    "sst_job_id = dbutils.widgets.get(\"sst_job_id\")\n",
    "\n",
    "# Delta Lake table details (read from job task values set by data ingestion task)\n",
    "catalog = DB_workspace\n",
    "read_schema = f\"{institution_id}_bronze\"\n",
    "write_schema = f\"{institution_id}_silver\"\n",
    "\n",
    "# Read DataFrames from Delta Lake tables (if Spark session is available)\n",
    "if spark_session:\n",
    "    df_course = pdp.schemas.RawPDPCourseDataSchema(\n",
    "        pdp.dataio.read_data_from_delta_table(\n",
    "            f\"{catalog}.{read_schema}.{sst_job_id}_course_dataset_validated\",\n",
    "            spark_session=spark_session,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_cohort = pdp.schemas.RawPDPCohortDataSchema(\n",
    "        pdp.dataio.read_data_from_delta_table(\n",
    "            f\"{catalog}.{read_schema}.{sst_job_id}_cohort_dataset_validated\",\n",
    "            spark_session=spark_session,\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    logging.warning(\"Spark session not initialized. Cannot read dataframes.\")\n",
    "    exit()  # Exit the script if the Spark session is not available.\n",
    "\n",
    "\n",
    "# Reading the parameters from the institution's configuration file\n",
    "cfg = configs.load_config(\n",
    "    f\"/Volumes/{DB_workspace}/{institution_id}_bronze/pdp_pipeline_internal/configuration_files/{institution_id}.toml\",\n",
    "    configs.PDPProjectConfigV2,\n",
    ")\n",
    "\n",
    "# Read preprocessing features\n",
    "min_passing_grade = cfg.preprocessing.features.min_passing_grade\n",
    "min_num_credits_full_time = cfg.preprocessing.features.min_num_credits_full_time\n",
    "course_level_pattern = cfg.preprocessing.features.course_level_pattern\n",
    "key_course_subject_areas = cfg.preprocessing.features.key_course_subject_areas\n",
    "key_course_ids = cfg.preprocessing.features.key_course_ids\n",
    "\n",
    "# Read preprocessing target params\n",
    "min_num_credits_checkin = cfg.preprocessing.target.params[\"min_num_credits_checkin\"]\n",
    "min_num_credits_target = cfg.preprocessing.target.params[\"min_num_credits_target\"]\n",
    "\n",
    "# Create student-term dataset\n",
    "df_student_terms = pdp.dataops.make_student_term_dataset(\n",
    "    df_cohort,\n",
    "    df_course,\n",
    "    min_passing_grade=min_passing_grade,\n",
    "    min_num_credits_full_time=min_num_credits_full_time,\n",
    "    course_level_pattern=course_level_pattern,\n",
    "    key_course_subject_areas=key_course_subject_areas,\n",
    "    key_course_ids=key_course_ids,\n",
    ")\n",
    "\n",
    "\n",
    "student_criteria = {\n",
    "    \"enrollment_type\": [\"FIRST-TIME\", \"RE-ADMIT\", \"TRANSFER-IN\"],\n",
    "    # \"enrollment_intensity_first_term\": [\"FULL-TIME\", \"PART-TIME\"], # Example, but commented out.\n",
    "    # \"credential_type_sought_year_1\": \"Associate's Degree\",  # Example, but commented out.\n",
    "}\n",
    "intensity_time_limits = [\n",
    "    (\"FULL-TIME\", 1.0, \"year\"),\n",
    "    (\"PART-TIME\", 1.0, \"year\"),\n",
    "]\n",
    "\n",
    "df_processed = pdp.targets.failure_to_earn_enough_credits_in_time_from_enrollment.make_inference_dataset(\n",
    "    df_student_terms,\n",
    "    min_num_credits_checkin=min_num_credits_checkin,\n",
    "    min_num_credits_target=min_num_credits_target,\n",
    "    student_criteria=student_criteria,\n",
    "    intensity_time_limits=intensity_time_limits,\n",
    ")\n",
    "\n",
    "\n",
    "# # Temporary workaround: Create and drop the 'target' label due to library issue.\n",
    "# # This should be replaced with the updated library function once available.\n",
    "# df_processed = pdp.targets.failure_to_earn_enough_credits_in_time_from_enrollment.make_labeled_dataset(\n",
    "#     df_student_terms,\n",
    "#     min_num_credits_checkin=min_num_credits_checkin,\n",
    "#     min_num_credits_target=min_num_credits_target,\n",
    "#     student_criteria=student_criteria,\n",
    "#     intensity_time_limits=intensity_time_limits,\n",
    "# )\n",
    "# df_processed.drop(\"target\", axis=1, inplace=True)  # Drop the temporary target column.\n",
    "\n",
    "# Save processed dataset to Delta Lake (if Spark session is available)\n",
    "if spark_session:\n",
    "    write_table_path = f\"{catalog}.{write_schema}.{sst_job_id}_processed_dataset\"\n",
    "    pdp.dataio.write_data_to_delta_table(\n",
    "        df_processed, write_table_path, spark_session=spark_session\n",
    "    )\n",
    "    logging.info(f\"Processed dataset written to: {write_table_path}\")\n",
    "else:\n",
    "    logging.warning(\"Spark session not initialized. Cannot write processed dataset.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_processing_pipeline_task",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
