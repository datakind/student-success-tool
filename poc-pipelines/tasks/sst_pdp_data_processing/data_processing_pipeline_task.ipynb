{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5c5d41a-57e2-4b2c-a1fb-129131bef244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script prepares data for inference in the Student Success Tool (SST) pipeline.\n",
    "\n",
    "It reads validated course and cohort data from Delta Lake tables, creates a student-term \n",
    "dataset, applies target variable logic (currently using a workaround due to library \n",
    "limitations), and saves the processed dataset back to a Delta Lake table.  It's \n",
    "designed to run within a Databricks environment.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "\n",
    "import mlflow\n",
    "from databricks.connect import DatabricksSession\n",
    "from databricks.sdk.runtime import dbutils\n",
    "\n",
    "from student_success_tool.analysis import pdp\n",
    "\n",
    "# Disable mlflow autologging (due to Databricks issues during feature selection)\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.WARNING)  # Ignore Databricks logger\n",
    "\n",
    "# Attempt to create a Spark session\n",
    "try:\n",
    "    spark_session = DatabricksSession.builder.getOrCreate()\n",
    "except Exception:\n",
    "    logging.warning('Unable to create Spark session; are you in a Databricks runtime?')\n",
    "    spark_session = None\n",
    "\n",
    "# Databricks workspace identifier\n",
    "DB_workspace = dbutils.widgets.get(\"DB_workspace\")\n",
    "\n",
    "# Input parameters from Databricks widgets\n",
    "institution_id = dbutils.widgets.get(\"institution_id\")\n",
    "sst_job_id = dbutils.widgets.get(\"sst_job_id\")\n",
    "\n",
    "# Delta Lake table details (read from job task values set by data ingestion task)\n",
    "catalog = DB_workspace\n",
    "read_schema = f\"{institution_id}_bronze\"\n",
    "write_schema = f\"{institution_id}_silver\"\n",
    "\n",
    "# Read DataFrames from Delta Lake tables (if Spark session is available)\n",
    "if spark_session:\n",
    "    df_course = pdp.schemas.RawPDPCourseDataSchema(\n",
    "        pdp.dataio.read_data_from_delta_table(\n",
    "            f\"{catalog}.{read_schema}.{sst_job_id}_course_dataset_validated\",\n",
    "            spark_session=spark_session,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_cohort = pdp.schemas.RawPDPCohortDataSchema(\n",
    "        pdp.dataio.read_data_from_delta_table(\n",
    "            f\"{catalog}.{read_schema}.{sst_job_id}_cohort_dataset_validated\",\n",
    "            spark_session=spark_session,\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    logging.warning(\"Spark session not initialized. Cannot read dataframes.\")\n",
    "    exit()  # Exit the script if the Spark session is not available.\n",
    "\n",
    "# TODO Update to take these parameterts from the institution .toml file\n",
    "# Default values for data processing parameters (This will take the values from .toml file)\n",
    "min_passing_grade = pdp.constants.DEFAULT_MIN_PASSING_GRADE\n",
    "min_num_credits_full_time = pdp.constants.DEFAULT_MIN_NUM_CREDITS_FULL_TIME\n",
    "course_level_pattern = pdp.constants.DEFAULT_COURSE_LEVEL_PATTERN\n",
    "key_course_subject_areas = None\n",
    "key_course_ids = None\n",
    "\n",
    "# Create student-term dataset\n",
    "df_student_terms = pdp.dataops.make_student_term_dataset(\n",
    "    df_cohort,\n",
    "    df_course,\n",
    "    min_passing_grade=min_passing_grade,\n",
    "    min_num_credits_full_time=min_num_credits_full_time,\n",
    "    course_level_pattern=course_level_pattern,\n",
    "    key_course_subject_areas=key_course_subject_areas,\n",
    "    key_course_ids=key_course_ids,\n",
    ")\n",
    "\n",
    "# TODO  Update to take these parameterts from the institution .toml file\n",
    "# Parameters for target variable calculation (This will take the values from .toml file)\n",
    "min_num_credits_checkin = 1.0\n",
    "min_num_credits_target = 1.0\n",
    "student_criteria = {\n",
    "    \"enrollment_type\": [\"FIRST-TIME\", \"RE-ADMIT\", \"TRANSFER-IN\"],\n",
    "    # \"enrollment_intensity_first_term\": [\"FULL-TIME\", \"PART-TIME\"], # Example, but commented out.\n",
    "    # \"credential_type_sought_year_1\": \"Associate's Degree\",  # Example, but commented out.\n",
    "}\n",
    "intensity_time_limits = [\n",
    "    (\"FULL-TIME\", 1.0, \"year\"),\n",
    "    (\"PART-TIME\", 1.0, \"year\"),\n",
    "]\n",
    "\n",
    "\n",
    "# TODO This code already works but this file/library needs to be updated in the repo: https://2415288190517010.0.gcp.databricks.com/editor/files/1043167876026327?o=2415288190517010\n",
    "# df_inference = pdp.targets.failure_to_earn_enough_credits_in_time_from_enrollment.make_inference_dataset(\n",
    "#     df_student_terms,\n",
    "#     min_num_credits_checkin=min_num_credits_checkin,\n",
    "#     min_num_credits_target=min_num_credits_target,\n",
    "#     student_criteria=student_criteria,\n",
    "#     intensity_time_limits=intensity_time_limits,\n",
    "# )\n",
    "\n",
    "\n",
    "# Temporary workaround: Create and drop the 'target' label due to library issue.\n",
    "# This should be replaced with the updated library function once available.\n",
    "df_processed = pdp.targets.failure_to_earn_enough_credits_in_time_from_enrollment.make_labeled_dataset(\n",
    "    df_student_terms,\n",
    "    min_num_credits_checkin=min_num_credits_checkin,\n",
    "    min_num_credits_target=min_num_credits_target,\n",
    "    student_criteria=student_criteria,\n",
    "    intensity_time_limits=intensity_time_limits,\n",
    ")\n",
    "df_processed.drop(\"target\", axis=1, inplace=True)  # Drop the temporary target column.\n",
    "\n",
    "# Save processed dataset to Delta Lake (if Spark session is available)\n",
    "if spark_session:\n",
    "    write_table_path = f\"{catalog}.{write_schema}.{sst_job_id}_processed_dataset\"\n",
    "    pdp.dataio.write_data_to_delta_table(\n",
    "        df_processed, write_table_path, spark_session=spark_session\n",
    "    )\n",
    "    logging.info(f\"Processed dataset written to: {write_table_path}\")\n",
    "else:\n",
    "    logging.warning(\"Spark session not initialized. Cannot write processed dataset.\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_processing_pipeline_task",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
