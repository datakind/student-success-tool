{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf9ebb0f-7638-4b96-b56d-488f223b8bd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script ingests course and cohort data for the Student Success Tool (SST) pipeline.\n",
    "\n",
    "It reads data from CSV files stored in a Google Cloud Storage (GCS) bucket, \n",
    "performs schema validation using the `pdp` library, and writes the validated data \n",
    "to Delta Lake tables in Databricks Unity Catalog.\n",
    "\n",
    "The script is designed to run within a Databricks environment, leveraging Databricks \n",
    "utilities for widget input, job task values, and Spark session management. It also \n",
    "handles cases where a Spark session cannot be initialized (e.g., running locally).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from databricks.connect import DatabricksSession\n",
    "from databricks.sdk.runtime import dbutils\n",
    "from google.cloud import storage\n",
    "\n",
    "from student_success_tool.analysis import pdp\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.WARNING)  # Ignore Databricks logger\n",
    "\n",
    "# Attempt to create a Spark session. Handles exceptions if not in Databricks.\n",
    "try:\n",
    "    spark_session = DatabricksSession.builder.getOrCreate()\n",
    "except Exception:\n",
    "    logging.warning(\"Unable to create Spark session; are you in a Databricks runtime?\")\n",
    "    spark_session = None\n",
    "\n",
    "# Input parameters (provided via Databricks widgets or job task values)\n",
    "DB_workspace = dbutils.widgets.get(\"DB_workspace\")  # Databricks workspace identifier\n",
    "\n",
    "institution_id = dbutils.widgets.get(\"institution_id\")\n",
    "sst_job_id = dbutils.widgets.get(\"sst_job_id\")\n",
    "medallion_level = \"bronze\"\n",
    "\n",
    "# Handle synthetic data generation case\n",
    "if dbutils.widgets.get(\"synthetic_needed\") == \"True\":\n",
    "    course_file_name = dbutils.jobs.taskValues.get(\n",
    "        taskKey=\"generate_synthetic_data\", key=\"course_file_name\"\n",
    "    )\n",
    "    cohort_file_name = dbutils.jobs.taskValues.get(\n",
    "        taskKey=\"generate_synthetic_data\", key=\"cohort_file_name\"\n",
    "    )\n",
    "else:\n",
    "    course_file_name = dbutils.widgets.get(\"course_file_name\")\n",
    "    cohort_file_name = dbutils.widgets.get(\"cohort_file_name\")\n",
    "\n",
    "\n",
    "# Define paths (using Unity Catalog volumes)\n",
    "internal_pipeline_path = f\"/Volumes/{DB_workspace}/{institution_id}_{medallion_level}/pdp_pipeline_internal/{sst_job_id}/raw_files/\"\n",
    "\n",
    "\n",
    "# Create internal pipeline directory\n",
    "os.makedirs(internal_pipeline_path, exist_ok=True)\n",
    "\n",
    "# Initialize GCS client\n",
    "storage_client = storage.Client()\n",
    "bucket_name = f\"{DB_workspace}_{institution_id}_sst_application\"\n",
    "\n",
    "bucket = storage_client.bucket(bucket_name)\n",
    "sst_container_folder = \"validated\"\n",
    "\n",
    "# Download course data from GCS\n",
    "course_blob_name = f\"{sst_container_folder}/{course_file_name}\"\n",
    "course_blob = bucket.blob(course_blob_name)\n",
    "course_blob.download_to_filename(f\"{internal_pipeline_path}{course_file_name}\")\n",
    "\n",
    "# Download cohort data from GCS\n",
    "cohort_blob_name = f\"{sst_container_folder}/{cohort_file_name}\"\n",
    "cohort_blob = bucket.blob(cohort_blob_name)\n",
    "cohort_blob.download_to_filename(f\"{internal_pipeline_path}{cohort_file_name}\")\n",
    "\n",
    "\n",
    "# Set path_volume (important for compatibility with Datakind's code)\n",
    "path_volume = internal_pipeline_path\n",
    "\n",
    "# Construct full file paths\n",
    "fpath_course = os.path.join(path_volume, course_file_name)\n",
    "fpath_cohort = os.path.join(path_volume, cohort_file_name)\n",
    "\n",
    "# Read data from CSV files into Pandas DataFrames and validate schema\n",
    "df_course = pdp.dataio.read_raw_pdp_course_data_from_file(\n",
    "    fpath_course, schema=pdp.schemas.RawPDPCourseDataSchema, dttm_format=None\n",
    ")\n",
    "df_cohort = pdp.dataio.read_raw_pdp_cohort_data_from_file(\n",
    "    fpath_cohort, schema=pdp.schemas.RawPDPCohortDataSchema\n",
    ")\n",
    "\n",
    "\n",
    "# Define Delta Lake table details\n",
    "catalog = DB_workspace\n",
    "write_schema = f\"{institution_id}_bronze\"  # TODO: Confirm medallion level usage\n",
    "\n",
    "\n",
    "# Write DataFrames to Delta Lake tables (only if Spark session is available)\n",
    "if spark_session:\n",
    "    pdp.dataio.write_data_to_delta_table(\n",
    "        df_course,\n",
    "        f\"{catalog}.{write_schema}.{sst_job_id}_course_dataset_validated\",\n",
    "        spark_session=spark_session,\n",
    "    )\n",
    "\n",
    "    pdp.dataio.write_data_to_delta_table(\n",
    "        df_cohort,\n",
    "        f\"{catalog}.{write_schema}.{sst_job_id}_cohort_dataset_validated\",\n",
    "        spark_session=spark_session,\n",
    "    )\n",
    "\n",
    "    # Verify Delta Lake write by reading data back\n",
    "    df_course_from_catalog = pdp.schemas.RawPDPCourseDataSchema(\n",
    "        pdp.dataio.read_data_from_delta_table(\n",
    "            f\"{catalog}.{write_schema}.{sst_job_id}_course_dataset_validated\",\n",
    "            spark_session=spark_session,\n",
    "        )\n",
    "    )\n",
    "    print(f\"Course DataFrame shape from catalog: {df_course_from_catalog.shape}\")\n",
    "\n",
    "    df_cohort_from_catalog = pdp.schemas.RawPDPCohortDataSchema(\n",
    "        pdp.dataio.read_data_from_delta_table(\n",
    "            f\"{catalog}.{write_schema}.{sst_job_id}_cohort_dataset_validated\",\n",
    "            spark_session=spark_session,\n",
    "        )\n",
    "    )\n",
    "    print(f\"Cohort DataFrame shape from catalog: {df_cohort_from_catalog.shape}\")\n",
    "else:\n",
    "    logging.warning(\n",
    "        \"Spark session not initialized. Skipping Delta Lake write and verification.\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_ingestion_pipeline_task",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
