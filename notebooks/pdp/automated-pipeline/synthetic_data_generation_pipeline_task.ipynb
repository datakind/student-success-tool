{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf9ebb0f-7638-4b96-b56d-488f223b8bd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script generates synthetic student cohort and course data using the `faker` \n",
    "and `pdp` libraries, and uploads the generated CSV files to a Google Cloud \n",
    "Storage (GCS) bucket.  It's designed to run within a Databricks environment, \n",
    "utilizing Databricks widgets for input parameters and job task values for passing\n",
    "filenames between tasks.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import faker\n",
    "import pandas as pd\n",
    "\n",
    "from databricks.sdk.runtime import dbutils\n",
    "from google.cloud import storage\n",
    "\n",
    "from student_success_tool.generation import pdp\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.WARNING)  # Ignore Databricks logger\n",
    "\n",
    "# Databricks workspace identifier\n",
    "DB_workspace = dbutils.widgets.get(\"DB_workspace\") \n",
    "\n",
    "# Input parameters from Databricks widgets\n",
    "institution_id = dbutils.widgets.get(\"institution_id\")\n",
    "normalize_col_names = None if dbutils.widgets.get(\"normalize_col_names\") == \"\" else dbutils.widgets.get(\"normalize_col_names\")\n",
    "avg_num_courses_per_student = int(dbutils.widgets.get(\"avg_num_courses_per_student\"))\n",
    "num_students = int(dbutils.widgets.get(\"num_students\"))\n",
    "\n",
    "\n",
    "# Optional seed for Faker (for reproducibility)\n",
    "seed = None if dbutils.widgets.get(\"seed\") == \"\" else int(dbutils.widgets.get(\"seed\"))\n",
    "\n",
    "sst_job_id = dbutils.widgets.get(\"sst_job_id\")\n",
    "\n",
    "# Define the save directory, defaulting to a location within Unity Catalog volumes\n",
    "if dbutils.widgets.get(\"save_dir\") == \"\":\n",
    "\n",
    "    bucket_name = f\"{DB_workspace}_{institution_id}_sst_application\"  # Bucket name based on institution ID\n",
    "else:\n",
    "    bucket_name = dbutils.widgets.get(\"save_dir\")\n",
    "\n",
    "logging.info(f\"Save directory: {bucket_name}\")\n",
    "logging.info(f\"normalize_col_names: {normalize_col_names}\")\n",
    "logging.info(f\"seed: {seed}\")\n",
    "logging.info(f\"num_students: {num_students}\")\n",
    "\n",
    "\n",
    "\n",
    "# Initialize Faker with optional seed and custom providers\n",
    "faker.Faker.seed(seed)\n",
    "FAKER = faker.Faker()\n",
    "FAKER.add_provider(pdp.raw_cohort.Provider)\n",
    "FAKER.add_provider(pdp.raw_course.Provider)\n",
    "\n",
    "# Initialize GCS client\n",
    "client = storage.Client()\n",
    "\n",
    "\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "# Generate cohort records\n",
    "cohort_records = [\n",
    "    FAKER.raw_cohort_record(\n",
    "        normalize_col_names=normalize_col_names, institution_id=institution_id\n",
    "    )\n",
    "    for _ in range(num_students)\n",
    "]\n",
    "\n",
    "# Generate course records (related to cohort records)\n",
    "course_records = [\n",
    "    FAKER.raw_course_record(\n",
    "        cohort_record, normalize_col_names=normalize_col_names\n",
    "    )\n",
    "    for cohort_record in cohort_records\n",
    "    for _ in range(\n",
    "        FAKER.randomize_nb_elements(avg_num_courses_per_student, min=1)  # Random number of courses per student\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create Pandas DataFrames\n",
    "df_cohort = pd.DataFrame(cohort_records)\n",
    "df_course = pd.DataFrame(course_records)\n",
    "\n",
    "logging.info(\n",
    "    \"Generated %s cohort records and %s course records\",\n",
    "    len(cohort_records),\n",
    "    len(course_records),\n",
    ")\n",
    "\n",
    "# Construct file names\n",
    "cohort_file_name = f'{institution_id}_{sst_job_id}_STUDENT_SEMESTER_AR_DEIDENTIFIED.csv'\n",
    "course_file_name = f'{institution_id}_{sst_job_id}_COURSE_LEVEL_AR_DEID.csv'\n",
    "\n",
    "# Construct full paths within the GCS bucket\n",
    "cohort_full_path_name = f'validated/{cohort_file_name}'\n",
    "course_full_path_name = f'validated/{course_file_name}'\n",
    "\n",
    "# Create GCS blob objects\n",
    "cohort_blob = bucket.blob(cohort_full_path_name)\n",
    "course_blob = bucket.blob(course_full_path_name)\n",
    "\n",
    "# Upload DataFrames to GCS as CSVs\n",
    "cohort_blob.upload_from_string(df_cohort.to_csv(header=True, index=False), 'text/csv')\n",
    "course_blob.upload_from_string(df_course.to_csv(header=True, index=False), 'text/csv')\n",
    "\n",
    "print(f\"Generated and uploaded files: {cohort_file_name} and {course_file_name} to GCS bucket: {bucket_name}\")\n",
    "\n",
    "# Set Databricks job task values for downstream tasks to access filenames\n",
    "dbutils.jobs.taskValues.set(key=\"cohort_file_name\", value=cohort_file_name)\n",
    "dbutils.jobs.taskValues.set(key=\"course_file_name\", value=course_file_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "synthetic_data_generation_pipeline_task",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
