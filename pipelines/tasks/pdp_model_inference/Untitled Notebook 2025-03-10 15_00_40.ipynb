{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b6168c73-d493-42a4-b7b4-0fbc80816915",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType, StringType, StructField, StructType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3302c1e7-da0d-48d6-a99c-b703124340f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e48ce08e-7c1a-4908-9ebd-e26eeb4b3bdd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script performs model inference for the Student Success Tool (SST) pipeline.\n",
    "\n",
    "It loads a pre-trained ML model from MLflow Model Registry,\n",
    "reads a processed dataset from Delta Lake, performs inference, calculates SHAP values,\n",
    "and writes the predictions back to Delta Lake.\n",
    "\n",
    "The script is designed to run within a Databricks environment as a job task, leveraging\n",
    "Databricks utilities for job task values and Spark session management.\n",
    "\n",
    "This is a POC script, it is advised to review and tests before using in production.\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary libraries\n",
    "import logging\n",
    "import os\n",
    "import argparse\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import typing as t\n",
    "import functools as ft\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from databricks.connect import DatabricksSession\n",
    "\n",
    "\n",
    "# Import project-specific modules\n",
    "import student_success_tool.dataio as dataio\n",
    "from student_success_tool.modeling import inference\n",
    "import student_success_tool.modeling as modeling\n",
    "from student_success_tool.schemas import pdp as schemas\n",
    "# from student_success_tool.pipeline_utils.plot import plot_shap_beeswarm\n",
    "# from pipelines.tasks.utils import emails\n",
    "\n",
    "\n",
    "# Disable mlflow autologging (prevents conflicts in Databricks environments)\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.WARNING)  # Suppress py4j logging\n",
    "\n",
    "\n",
    "class ModelInferenceTask:\n",
    "    \"\"\"Encapsulates the model inference logic for the SST pipeline.\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initializes the ModelInferenceTask.\"\"\"\n",
    "        # self.args = args\n",
    "        self.spark_session = self.get_spark_session()\n",
    "        self.cfg = self.read_config(toml_file_path)  \n",
    "        # print(self.args)\n",
    "        # print(f\"{job_root_dir}/ext/\")\n",
    "\n",
    "\n",
    "    def get_spark_session(self) -> DatabricksSession | None:\n",
    "        \"\"\"\n",
    "        Attempts to create a Spark session.\n",
    "        Returns:\n",
    "            DatabricksSession | None: A Spark session if successful, None otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            spark_session = DatabricksSession.builder.getOrCreate()\n",
    "            logging.info(\"Spark session created successfully.\")\n",
    "            return spark_session\n",
    "        except Exception:\n",
    "            logging.error(\"Unable to create Spark session.\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def read_config(self, toml_file_path: str) -> schemas.PDPProjectConfig:\n",
    "        \"\"\"Reads the institution's model's configuration file.\"\"\"\n",
    "        try:\n",
    "            cfg = dataio.read_config(toml_file_path, schema=schemas.PDPProjectConfig)\n",
    "            return cfg\n",
    "        except FileNotFoundError:\n",
    "            logging.error(\"Configuration file not found at %s\", toml_file_path)\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error reading configuration file: %e\", e)\n",
    "            raise\n",
    "\n",
    "\n",
    "    def load_mlflow_model(self) -> mlflow.pyfunc.PyFuncModel:\n",
    "        \"\"\"Loads the MLflow model.\"\"\"\n",
    "        model_schema = f\"{databricks_institution_name}_gold\"\n",
    "        model_uri = f\"runs:/{self.cfg.models['graduation'].run_id}/model\"\n",
    "        print(model_uri)\n",
    "\n",
    "        try:\n",
    "            load_model_func = {\n",
    "                \"sklearn\": mlflow.sklearn.load_model,\n",
    "                \"xgboost\": mlflow.xgboost.load_model,\n",
    "                \"lightgbm\": mlflow.lightgbm.load_model,\n",
    "                \"pyfunc\": mlflow.pyfunc.load_model,  # Default\n",
    "            }.get(model_type, mlflow.pyfunc.load_model)\n",
    "            model = load_model_func(model_uri)\n",
    "            logging.info(\"MLflow '%s' model loaded from '%s'\", model_type, model_uri)\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error loading MLflow model: %s\", e)\n",
    "            raise  # Critical error; re-raise to halt execution\n",
    "\n",
    "\n",
    "    def predict(self, model: mlflow.pyfunc.PyFuncModel, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Performs inference and adds predictions to the DataFrame.\"\"\"\n",
    "        try:\n",
    "            model_feature_names = model.named_steps[\"column_selector\"].get_params()[\"cols\"]\n",
    "        except AttributeError:\n",
    "             model_feature_names = model.metadata.get_input_schema().input_names()\n",
    "\n",
    "        df_serving = df[model_feature_names]\n",
    "\n",
    "        df_predicted = df_serving.copy()\n",
    "        df_predicted[\"predicted_label\"] = model.predict(df_serving)\n",
    "        try:\n",
    "            df_predicted[\"predicted_prob\"] = model.predict_proba(df_serving)[:, 1]\n",
    "        except AttributeError:\n",
    "            logging.error(\"Model does not have predict_proba method.  Skipping.\")\n",
    "            raise\n",
    "        return df_predicted\n",
    "\n",
    "    def write_data_to_delta(self, df: pd.DataFrame, table_name_suffix: str):\n",
    "        \"\"\"Writes a DataFrame to a Delta Lake table.\"\"\"\n",
    "        write_schema = f\"{databricks_institution_name}_silver\"\n",
    "        table_path = f\"{DB_workspace}.{write_schema}.{db_run_id}_{table_name_suffix}\"\n",
    "\n",
    "        try:\n",
    "            dataio.to_delta_table(df, table_path, spark_session=self.spark_session)\n",
    "            logging.info(\"%s data written to: %s\", table_name_suffix.capitalize(), table_path)\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error writing %s data to Delta Lake: %s\", table_name_suffix, e)\n",
    "            raise # Critical, prevent further execution.\n",
    "\n",
    "    @staticmethod\n",
    "    def predict_proba(\n",
    "        X: pd.DataFrame,\n",
    "        model: mlflow.pyfunc.PyFuncModel,\n",
    "        feature_names: t.Optional[list[str]] = None,\n",
    "        pos_label: t.Optional[bool | str] = None,\n",
    "    ) -> np.ndarray:\n",
    "        \"\"\"Predicts probabilities using the provided model.  Handles data prep.\"\"\"\n",
    "\n",
    "        if feature_names is None:\n",
    "            feature_names = model.named_steps[\"column_selector\"].get_params()[\"cols\"]\n",
    "        if not isinstance(X, pd.DataFrame):\n",
    "            X = pd.DataFrame(data=X, columns=feature_names)\n",
    "        # else: # This check seems unnecessary and potentially incorrect.\n",
    "        #     assert X.shape[1] == len(feature_names)  # Check *number* of columns.\n",
    "        pred_probs = model.predict_proba(X)\n",
    "        if pos_label is not None:\n",
    "            return pred_probs[:, model.classes_.tolist().index(pos_label)]\n",
    "        else:\n",
    "            return pred_probs\n",
    "\n",
    "\n",
    "    def calculate_shap_values(\n",
    "        self,\n",
    "        model: mlflow.pyfunc.PyFuncModel,\n",
    "        df_processed: pd.DataFrame,\n",
    "        model_feature_names: list[str]\n",
    "    ) -> pd.DataFrame | None:\n",
    "        \"\"\"Calculates SHAP values.\"\"\"\n",
    "        try:\n",
    "            # --- Load features table ---\n",
    "            features_table = dataio.read_features_table(\"assets/pdp/features_table.toml\")\n",
    "\n",
    "            # --- SHAP Values Calculation ---\n",
    "            # TODO: Consider saving the explainer during training.\n",
    "            shap_ref_data_size = 200  # Consider getting from config.\n",
    "\n",
    "            experiment_id = self.cfg.models[\"graduation\"].experiment_id # Consider refactoring this\n",
    "            df_train = modeling.evaluation.extract_training_data_from_model(experiment_id)\n",
    "            train_mode = df_train.mode().iloc[0]  # Use .iloc[0] for single row\n",
    "            df_ref = (\n",
    "                df_train.sample(\n",
    "                    n=min(shap_ref_data_size, df_train.shape[0]),\n",
    "                    random_state=self.cfg.random_state,\n",
    "                )\n",
    "                .fillna(train_mode)\n",
    "                .loc[:, model_feature_names]\n",
    "            )\n",
    "\n",
    "            explainer_object = shap.explainers.KernelExplainer(\n",
    "                ft.partial(\n",
    "                    self.predict_proba,  # Use the static method\n",
    "                    model=model,\n",
    "                    feature_names=model_feature_names,\n",
    "                    pos_label=self.cfg.pos_label,\n",
    "                ),\n",
    "                df_ref,\n",
    "                link=\"identity\",\n",
    "            )\n",
    "\n",
    "            # shap_values = explainer_object(df_processed[model_feature_names])\n",
    "\n",
    "            def create_explanation(model, data_chunk, explainer_object):\n",
    "                explanation = explainer_object(data_chunk)\n",
    "                return explanation\n",
    "            \n",
    "            \n",
    "\n",
    "            def parallel_explanations_joblib(model, X, explainer_object, n_jobs=-1):\n",
    "                chunks = np.array_split(X, len(X) // 4)\n",
    "                # results = Parallel(n_jobs=n_jobs)(delayed(create_explanation)(model, chunk, explainer_object) for chunk in chunks)\n",
    "                results = Parallel(n_jobs=n_jobs)(delayed(lambda model, chunk, explainer: explainer(chunk))(model, chunk, explainer_object) for chunk in chunks)\n",
    "\n",
    "                combined_values = np.concatenate([r.values for r in results], axis=0)\n",
    "\n",
    "                combined_data = np.concatenate([r.data for r in results], axis=0)\n",
    "\n",
    "\n",
    "                combined_explanation = shap.Explanation(values=combined_values, data=combined_data, feature_names=model_feature_names)\n",
    "\n",
    "                return combined_explanation\n",
    "\n",
    "            shap_values = parallel_explanations_joblib(model, df_processed[model_feature_names], explainer_object)\n",
    "\n",
    "            # shap_schema = StructType(\n",
    "            #     [StructField(self.cfg.student_id_col, StringType(), nullable=False)]\n",
    "            #     + [StructField(col, FloatType(), nullable=False) for col in model_feature_names]\n",
    "            # )\n",
    "\n",
    "            # df_shap_values = (\n",
    "            #     self.spark_session.createDataFrame(\n",
    "            #         df_processed.reindex(\n",
    "            #             columns=model_feature_names + [self.cfg.student_id_col]\n",
    "            #         )\n",
    "            #     )\n",
    "            #     .repartition(self.spark_session.sparkContext.defaultParallelism)\n",
    "            #     .mapInPandas(\n",
    "            #         ft.partial(\n",
    "            #             inference.calculate_shap_values_spark_udf,\n",
    "            #             student_id_col=self.cfg.student_id_col,\n",
    "            #             model_features=model_feature_names,\n",
    "            #             explainer=explainer,\n",
    "            #             mode=train_mode,\n",
    "            #         ),\n",
    "            #         schema=shap_schema,\n",
    "            #     )\n",
    "            #     .toPandas()\n",
    "            #     .set_index(self.cfg.student_id_col)\n",
    "            #     .reindex(df_processed[self.cfg.student_id_col])\n",
    "            #     .reset_index(drop=False)\n",
    "            # )\n",
    "\n",
    "\n",
    "            return shap_values\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error during SHAP value calculation: %s\", e)\n",
    "            raise\n",
    "            \n",
    "\n",
    "\n",
    "    def get_top_features_for_display(self, df_serving, unique_ids, df_predicted, shap_values, model_feature_names):\n",
    "        \"\"\"\n",
    "        Selects top features to display and store\n",
    "        \"\"\"\n",
    "        if not self.spark_session:\n",
    "            logging.error(\"Spark session not initialized. Cannot post process shap values.\")\n",
    "            return None\n",
    "\n",
    "        # --- Load features table ---\n",
    "        features_table = dataio.read_features_table(\"assets/pdp/features_table.toml\")\n",
    "\n",
    "        # --- Inference Parameters ---\n",
    "        inference_params = {\n",
    "            \"num_top_features\": 5,\n",
    "            \"min_prob_pos_label\": 0.5,\n",
    "        }\n",
    "\n",
    "        pred_probs = df_predicted[\"predicted_prob\"]\n",
    "        # --- Feature Selection for Display ---\n",
    "        try:\n",
    "            result = inference.select_top_features_for_display(\n",
    "                df_serving,\n",
    "                unique_ids,\n",
    "                pred_probs,\n",
    "                shap_values.values,\n",
    "                n_features=inference_params[\"num_top_features\"],\n",
    "                features_table=features_table,\n",
    "                needs_support_threshold_prob=inference_params[\"min_prob_pos_label\"],\n",
    "            )\n",
    "            return result\n",
    "\n",
    "        except Exception as e:\n",
    "            logging.error(\"Error top features to display: %s\", e)\n",
    "            return None\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Executes the model inference pipeline.\"\"\"\n",
    "        print(processed_dataset_path)\n",
    "        df_processed = dataio.from_delta_table(processed_dataset_path, spark_session=self.spark_session)\n",
    "        unique_ids = df_processed[self.cfg.student_id_col]\n",
    "        \n",
    "        model = self.load_mlflow_model()\n",
    "        model_feature_names = model.named_steps[\"column_selector\"].get_params()[\"cols\"]\n",
    "        \n",
    "         # --- Email notify users ---\n",
    "        # Uncomment below once we want to enable CC'ing to DK's email.\n",
    "        # emails.send_inference_kickoff_email(SENDER_EMAIL, [notif_email], [DK_CC_EMAIL], MANDRILL_USERNAME, MANDRILL_PASSWORD)\n",
    "        # emails.send_inference_kickoff_email(\n",
    "        #     SENDER_EMAIL, [notif_email], [], MANDRILL_USERNAME, MANDRILL_PASSWORD\n",
    "        # )\n",
    "\n",
    "\n",
    "        df_predicted = self.predict(model, df_processed)\n",
    "        # self.write_data_to_delta(df_predicted, \"predicted_dataset\")\n",
    "\n",
    "        # --- SHAP Values Calculation ---\n",
    "        shap_values = self.calculate_shap_values(model, df_processed, model_feature_names)\n",
    "        # with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\n",
    "        #     # Distribute the data to the processes and collect the results\n",
    "        #     results = pool.map(self.calculate_shap_values(model, df_processed, model_feature_names), df_processed)\n",
    "\n",
    "\n",
    "        if shap_values is not None:  # Proceed only if SHAP values were calculated\n",
    "            # --- SHAP Summary Plot ---\n",
    "            # shap_fig = plot_shap_beeswarm(shap_values)\n",
    "            \n",
    "            shap_top_features_results = self.get_top_features_for_display(df_processed, unique_ids, df_predicted, shap_values, model_feature_names)\n",
    "            # --- Save Results to ext/ folder in Gold volume. ---\n",
    "            if shap_top_features_results is not None:\n",
    "                # Specify the folder for the output files to be stored.\n",
    "                result_path = f\"{job_root_dir}/ext/\"\n",
    "                os.makedirs(result_path, exist_ok=True)\n",
    "                print('result_path:', result_path)\n",
    "\n",
    "                # Write the DataFrame to Unity Catalog table\n",
    "                self.write_data_to_delta(shap_top_features_results, \"shap_results_dataset\")\n",
    "\n",
    "                # Write the DataFrame to CSV in the specified volume\n",
    "                spark_df = self.spark_session.createDataFrame(shap_top_features_results)\n",
    "                spark_df.coalesce(1).write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(result_path + \"inference_output\")\n",
    "                # Write the SHAP chart png to the volume\n",
    "                # shap_fig.savefig(result_path + \"shap_chart.png\", bbox_inches=\"tight\")\n",
    "            else:\n",
    "                logging.error(\"Empty Shap results, cannot create the SHAP chart and table\")\n",
    "                raise Exception(\"Empty Shap results, cannot create the SHAP chart and table\")\n",
    "\n",
    "        # --- Write Inference Dataset --- (This was missing, but good to have)\n",
    "        self.write_data_to_delta(df_processed[model_feature_names], \"inference_dataset\")\n",
    "\n",
    "# def parse_arguments() -> argparse.Namespace:\n",
    "#     \"\"\"Parses command line arguments.\"\"\"\n",
    "#     parser = argparse.ArgumentParser(\n",
    "#         description=\"Perform model inference for the SST pipeline.\",\n",
    "#         formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n",
    "#     )\n",
    "#     parser.add_argument(\"--DB_workspace\", type=str, required=True, help=\"Databricks workspace identifier\")\n",
    "#     parser.add_argument(\"--databricks_institution_name\", type=str, required=True, help=\"Databricks institution name\")\n",
    "#     parser.add_argument(\"--db_run_id\", type=str, required=True, help=\"Databricks run ID\")\n",
    "#     parser.add_argument(\"--model_name\", type=str, required=True, help=\"Model name\")\n",
    "#     parser.add_argument(\"--model_type\", type=str, required=True, help=\"Model type\")\n",
    "#     parser.add_argument(\"--job_root_dir\", required=True, type=str, help=\"Folder path to store job output files\")\n",
    "#     parser.add_argument(\"--toml_file_path\", type=str, required=True, help=\"Path to configuration file\")\n",
    "#     parser.add_argument(\"--processed_dataset_path\", type=str, required=True, help=\"Path to processed dataset table\")\n",
    "#     return parser.parse_args()\n",
    "\n",
    "# if __name__ == \"__main__\":\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc45935f-52f5-4a9b-9c06-7a7ca405265d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/datakind/student-success-tool.git@pdp-inference-pipeline-refactor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88a7071b-3f26-4fd0-a72a-27aa2d1c4fff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark_session = DatabricksSession.builder.getOrCreate()\n",
    "course_dataset = dataio.from_delta_table(\"dev_sst_02.uni_of_crystal_testing_bronze.1080441261747080_course_dataset_validated\", spark_session=spark_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bacc4ccc-1efa-4ff0-851c-d9c0d9576c2a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "course_dataset.course_name[course_dataset.course_name==\"ENGL101\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c92fdc58-8c9f-4f26-8956-8bbd363ed7a7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "processed_dataset_path = \"dev_sst_02.uni_of_crystal_testing_silver.1080441261747080_processed_dataset\"\n",
    "spark_session = DatabricksSession.builder.getOrCreate()\n",
    "df_processed = dataio.from_delta_table(processed_dataset_path, spark_session=spark_session)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8c3837c-5573-40c6-b4b8-7f5567b6928a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "load_model_func = {\n",
    "                \"sklearn\": mlflow.sklearn.load_model,\n",
    "                \"xgboost\": mlflow.xgboost.load_model,\n",
    "                \"lightgbm\": mlflow.lightgbm.load_model,\n",
    "                \"pyfunc\": mlflow.pyfunc.load_model,  # Default\n",
    "            }.get(\"sklearn\", mlflow.pyfunc.load_model)\n",
    "model = load_model_func(\"runs:/0b12e0d2eda648a88031636cc21749b6/model\")\n",
    "# model = mlflow.pyfunc.load_model(\"runs:/0b12e0d2eda648a88031636cc21749b6/model\", \"sklearn\")\n",
    "model_feature_names = model.named_steps[\"column_selector\"].get_params()[\"cols\"]\n",
    "shap_ref_data_size = 200\n",
    "experiment_id = \"461684477982665\" # Consider refactoring this\n",
    "df_train = modeling.evaluation.extract_training_data_from_model(experiment_id)\n",
    "train_mode = df_train.mode().iloc[0]  # Use .iloc[0] for single row\n",
    "df_ref = (\n",
    "    df_train.sample(\n",
    "        n=min(shap_ref_data_size, df_train.shape[0]),\n",
    "        random_state=1234,\n",
    "    )\n",
    "    .fillna(train_mode)\n",
    "    .loc[:, model_feature_names]\n",
    ")\n",
    "\n",
    "\n",
    "def predict_proba(\n",
    "    X: pd.DataFrame,\n",
    "    model: mlflow.pyfunc.PyFuncModel,\n",
    "    feature_names: t.Optional[list[str]] = None,\n",
    "    pos_label: t.Optional[bool | str] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Predicts probabilities using the provided model.  Handles data prep.\"\"\"\n",
    "\n",
    "    if feature_names is None:\n",
    "        feature_names = model.named_steps[\"column_selector\"].get_params()[\"cols\"]\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(data=X, columns=feature_names)\n",
    "    # else: # This check seems unnecessary and potentially incorrect.\n",
    "    #     assert X.shape[1] == len(feature_names)  # Check *number* of columns.\n",
    "    pred_probs = model.predict_proba(X)\n",
    "    if pos_label is not None:\n",
    "        return pred_probs[:, model.classes_.tolist().index(pos_label)]\n",
    "    else:\n",
    "        return pred_probs\n",
    "    \n",
    "explainer_object = shap.explainers.KernelExplainer(\n",
    "    ft.partial(\n",
    "        predict_proba,  # Use the static method\n",
    "        model=model,\n",
    "        feature_names=model_feature_names,\n",
    "        pos_label=True,\n",
    "    ),\n",
    "    df_ref,\n",
    "    link=\"identity\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d2ea47eb-742b-4d79-b603-b8d0c4cc1a61",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def create_explanation(model, data_chunk, explainer_object):\n",
    "    explanation = explainer_object(data_chunk)\n",
    "    return explanation\n",
    "\n",
    "import os\n",
    "\n",
    "num_cpus = os.cpu_count()\n",
    "\n",
    "def parallel_explanations_joblib(model, X, explainer_object, n_jobs=-1):\n",
    "    chunks = np.array_split(X, len(X) // 4)\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(lambda model, chunk, explainer: explainer(chunk))(model, chunk, explainer_object) for chunk in chunks)\n",
    "\n",
    "\n",
    "    combined_values = np.concatenate([r.values for r in results], axis=0)\n",
    "    combined_data = np.concatenate([r.data for r in results], axis=0)\n",
    "\n",
    "    combined_explanation = shap.Explanation(values=combined_values, data=combined_data, feature_names=results[0].feature_names)\n",
    "\n",
    "    return combined_explanation\n",
    "\n",
    "def create_explanation(model, data_chunk, explainer_object):\n",
    "    explanation = explainer_object(data_chunk)\n",
    "    return explanation\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2722abea-906f-44d1-9d7c-ddd8855878ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shap_values = parallel_explanations_joblib(model, df_processed[model_feature_names], explainer_object, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a612f1cd-529e-485a-99fb-6a9454714b4c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def parallel_shap_spark(spark, model, data, explainer_object, feature_names):\n",
    "    \"\"\"Parallelizes SHAP calculations using Spark with broadcast.\"\"\"\n",
    "\n",
    "    broadcast_explainer = spark.sparkContext.broadcast(explainer_object)\n",
    "\n",
    "    def calculate_shap_partition(iterator):\n",
    "        explainer = broadcast_explainer.value #get the broadcasted explainer.\n",
    "        for row in iterator:\n",
    "            chunk = np.array(row.features)\n",
    "            explanation = explainer(chunk)\n",
    "            yield (explanation.values, explanation.data)\n",
    "\n",
    "    spark_df = spark.createDataFrame([(row,) for row in data.tolist()], [\"features\"])\n",
    "    results_rdd = spark_df.rdd.mapPartitions(calculate_shap_partition)\n",
    "    collected_results = results_rdd.collect()\n",
    "\n",
    "    combined_values = np.concatenate([r[0] for r in collected_results], axis=0)\n",
    "    combined_data = np.concatenate([r[1] for r in collected_results], axis=0)\n",
    "    combined_explanation = shap.Explanation(values=combined_values, data=combined_data, feature_names=feature_names)\n",
    "\n",
    "    return combined_explanation\n",
    "\n",
    "# Example Usage:\n",
    "# spark = SparkSession.builder.appName(\"ShapSpark\").getOrCreate()\n",
    "\n",
    "#Example Model and Data\n",
    "# def example_model(X):\n",
    "#     return np.sum(X, axis=1)\n",
    "# data = np.random.rand(1000, 10)\n",
    "# background_data = shap.sample(data, 100)\n",
    "# explainer = shap.KernelExplainer(model, background_data)\n",
    "# feature_names = [\"feature_\" + str(i) for i in range(data.shape[1])]\n",
    "\n",
    "result = parallel_shap_spark(spark_session, model, df_processed[model_feature_names], explainer_object, model_feature_names)\n",
    "print(result.shape)\n",
    "\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "262aa737-ac6e-4af0-8a4f-1f7099a4c9f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "explainer_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8c2f7e6d-8277-45df-b658-d4de45c9bbed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sparkContext.broadcast(explainer_object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "00678a6e-c208-45a0-ae14-c9619617b990",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "shap_values = explainer_object(df_processed[model_feature_names])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a7283067-299b-4070-b202-ab11dcb375a4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "task = ModelInferenceTask()\n",
    "task.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b11289e-27e4-44e7-a18d-128fa754115e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DB_workspace=\"dev_sst_02\"\n",
    "databricks_institution_name=\"uni_of_crystal_testing\"\n",
    "db_run_id=\"206958269704514\"\n",
    "model_name=\"latest_enrollment_model\"\n",
    "model_type=\"sklearn\"\n",
    "job_root_dir=f\"/Volumes/{DB_workspace}/{databricks_institution_name}_gold/gold_volume/inference_jobs/{db_run_id}\"\n",
    "processed_dataset_path=f\"{DB_workspace}.{databricks_institution_name}_silver.{db_run_id}_processed_dataset\"\n",
    "toml_file_path=f\"/Volumes/{DB_workspace}/{databricks_institution_name}_gold/gold_volume/configuration_files/{databricks_institution_name}_latest_enrollment_model_configuration_file.toml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed7ad802-026f-4d16-becf-8161c6874383",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cf5aab9-6c88-4163-9e24-383a3cf98e36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def my_function(data):\n",
    "    # Process the data\n",
    "    return result\n",
    "\n",
    "df_predicted = self.predict(model, df_processed)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    # Prepare the data for processing\n",
    "    data_list = [data1, data2, data3, data4]\n",
    "\n",
    "    # Create a pool of processes, using all available cores\n",
    "    with multiprocessing.Pool(multiprocessing.cpu_count()) as pool:\n",
    "        # Distribute the data to the processes and collect the results\n",
    "        results = pool.map(self.predict(), model, dataframe)\n",
    "\n",
    "    # Process the results\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccb25963-7b16-49bb-b6cd-6ad2d88e1214",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dir(shap.Explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4257afe-a1c9-4bc0-8e0d-396bce944279",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0559b32f-ed2e-4b8a-a983-03ee3d4613f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "def model(X):\n",
    "    return np.sum(X, axis=1)\n",
    "\n",
    "X = np.random.rand(1000, 10)\n",
    "\n",
    "def create_explanation(model, data_chunk, background_data):\n",
    "    explainer = shap.KernelExplainer(model, background_data)\n",
    "    explanation = explainer(data_chunk)\n",
    "    return explanation\n",
    "\n",
    "def parallel_explanations_joblib(model, X, background_data, n_jobs=-1):\n",
    "    chunks = np.array_split(X, len(X) // 100)\n",
    "    results = Parallel(n_jobs=n_jobs)(delayed(create_explanation)(model, chunk, background_data) for chunk in chunks)\n",
    "\n",
    "    combined_values = np.concatenate([r.values for r in results], axis=0)\n",
    "    combined_data = np.concatenate([r.data for r in results], axis=0)\n",
    "\n",
    "    combined_explanation = shap.Explanation(values=combined_values, data=combined_data, feature_names=results[0].feature_names)\n",
    "\n",
    "    return combined_explanation\n",
    "\n",
    "background_data = shap.sample(X, 100)\n",
    "combined_explanation = parallel_explanations_joblib(model, X, background_data)\n",
    "\n",
    "print(combined_explanation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5021757e-0842-4ccc-9160-c4794bd446ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import multiprocessing\n",
    "from functools import partial\n",
    "\n",
    "# Example model\n",
    "def model(X):\n",
    "    return np.sum(X, axis=1)\n",
    "\n",
    "# Example data\n",
    "X = np.random.rand(1000, 10)\n",
    "\n",
    "def create_explanation(model, data_chunk, background_data):\n",
    "    explainer = shap.KernelExplainer(model, background_data)\n",
    "    explanation = explainer(data_chunk)\n",
    "    return explanation\n",
    "\n",
    "def parallel_explanations(model, X, background_data, n_processes=None):\n",
    "    if n_processes is None:\n",
    "        n_processes = multiprocessing.cpu_count()\n",
    "\n",
    "    chunks = np.array_split(X, n_processes)\n",
    "    pool = multiprocessing.Pool(processes=n_processes)\n",
    "    partial_create = partial(create_explanation, model, background_data=background_data)\n",
    "    results = pool.map(partial_create, chunks)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "    # Combine the Explanation objects (if needed)\n",
    "    combined_explanation = shap.Explanation.combine(results)\n",
    "    return combined_explanation\n",
    "\n",
    "# Example usage\n",
    "background_data = shap.sample(X, 100)\n",
    "combined_explanation = parallel_explanations(model, X, background_data)\n",
    "\n",
    "print(combined_explanation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "483e41ef-b59e-411f-a3b5-c3f151f67faa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Untitled Notebook 2025-03-10 15_00_40",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
