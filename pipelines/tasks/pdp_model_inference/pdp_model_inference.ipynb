{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6107cdd3-68e1-4242-8831-d3c06f40338b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script performs model inference for the Student Success Tool (SST) pipeline.\n",
    "\n",
    "It loads a pre-trained ML model from MLflow Model Registry, \n",
    "reads a processed dataset from Delta Lake, performs inference, calculates SHAP values, \n",
    "and writes the predictions back to Delta Lake.  \n",
    "\n",
    "The notebook is designed to run within a Databricks environment as a job task, leveraging Databricks \n",
    "utilities for widget input, job task values, and Spark session management.\n",
    "\n",
    "This is a POC notebook, it is advised to refactor to .py and add tests before using in production.\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary libraries\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import typing as t\n",
    "import functools as ft\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from databricks.connect import DatabricksSession\n",
    "from databricks.sdk.runtime import dbutils  # noqa: F401\n",
    "from pyspark.sql.types import FloatType, StringType, StructField, StructType\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from email.headerregistry import Address\n",
    "import shap\n",
    "import matplotlib.colors as mcolors\n",
    "\n",
    "# Import project-specific modules\n",
    "import student_success_tool.dataio as dataio\n",
    "from student_success_tool.modeling import inference\n",
    "import student_success_tool.modeling as modeling\n",
    "from student_success_tool.schemas import pdp as schemas\n",
    "from pipelines.tasks.utils import emails\n",
    "from student_success_tool.pipeline_utils.plot import plot_shap_beeswarm\n",
    "\n",
    "\n",
    "# Disable mlflow autologging (prevents conflicts in Databricks environments)\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.WARNING)  # Suppress py4j logging\n",
    "\n",
    "# --- Spark Session Initialization ---\n",
    "try:\n",
    "    spark_session = DatabricksSession.builder.getOrCreate()\n",
    "except Exception:\n",
    "    logging.warning(\"Unable to create Spark session; are you in a Databricks runtime?\")\n",
    "    spark_session = None\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input Parameters ( from Databricks widgets)\n",
    "DB_workspace = dbutils.widgets.get(\"DB_workspace\")\n",
    "catalog = DB_workspace\n",
    "institution_name = dbutils.widgets.get(\"databricks_institution_name\")\n",
    "sst_job_id = dbutils.widgets.get(\"db_run_id\")\n",
    "model_name = dbutils.widgets.get(\"model_name\")\n",
    "model_version = dbutils.widgets.get(\"version_id\")\n",
    "model_type = dbutils.widgets.get(\"model_type\")\n",
    "notif_email = dbutils.widgets.get(\"notification_email\")\n",
    "\n",
    "# Secrets from Databricks\n",
    "w = WorkspaceClient()\n",
    "MANDRILL_USERNAME = w.dbutils.secrets.get(scope=\"sst\", key=\"MANDRILL_USERNAME\")\n",
    "MANDRILL_PASSWORD = w.dbutils.secrets.get(scope=\"sst\", key=\"MANDRILL_PASSWORD\")\n",
    "SENDER_EMAIL = Address(\"Datakind Info\", \"help\", \"datakind.org\")\n",
    "DK_CC_EMAIL = \"education@datakind.org\"\n",
    "\n",
    "# --- Unity caatalog schemas ---\n",
    "read_schema = f\"{institution_name}_silver\"\n",
    "write_schema = f\"{institution_name}_silver\"\n",
    "model_schema = f\"{institution_name}_gold\"\n",
    "\n",
    "\n",
    "# --- Insititution Configuration ---\n",
    "cfg = dataio.read_config(\n",
    "    f\"/Volumes/{DB_workspace}/{institution_name}_gold/gold_volume/configuration_files/{institution_name}_{model_name}_configuration_file.toml\",\n",
    "    schema=schemas.PDPProjectConfig,\n",
    ")\n",
    "\n",
    "# --- Model Configuration ---\n",
    "# model_run_id = \"890b54cf68b147d7a55f515f61d5bfb2\"\n",
    "# experiment_id = \"1510364684601785\"\n",
    "experiment_id = cfg.models[\"graduation\"].experiment_id\n",
    "model_run_id = cfg.models[\"graduation\"].run_id\n",
    "model_uri = f\"models:/{catalog}.{model_schema}.{model_name}/1\"\n",
    "\n",
    "\n",
    "# --- Load features table ---\n",
    "features_table = dataio.read_features_table(\"assets/pdp/features_table.toml\")\n",
    "\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def mlflow_load_model(model_uri: str, model_type: str):\n",
    "    \"\"\"Loads an MLflow model based on its type.\"\"\"\n",
    "\n",
    "    # Dictionary mapping model types to loading functions\n",
    "    load_model_func = {\n",
    "        \"sklearn\": mlflow.sklearn.load_model,\n",
    "        \"xgboost\": mlflow.xgboost.load_model,\n",
    "        \"lightgbm\": mlflow.lightgbm.load_model,\n",
    "        \"pyfunc\": mlflow.pyfunc.load_model,  # Default\n",
    "    }.get(model_type, mlflow.pyfunc.load_model)\n",
    "\n",
    "    model = load_model_func(model_uri)\n",
    "    logging.info(\"MLflow '%s' model loaded from '%s'\", model_type, model_uri)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_proba(\n",
    "    X,\n",
    "    model,\n",
    "    *,\n",
    "    feature_names: t.Optional[list[str]] = None,\n",
    "    pos_label: t.Optional[bool | str] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Predicts probabilities using the provided model.\"\"\"\n",
    "\n",
    "    if feature_names is None:\n",
    "        feature_names = model.named_steps[\"column_selector\"].get_params()[\"cols\"]\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(data=X, columns=feature_names)\n",
    "    else:\n",
    "        assert X.shape == len(feature_names)\n",
    "    pred_probs = model.predict_proba(X)\n",
    "    if pos_label is not None:\n",
    "        return pred_probs[:, model.classes_.tolist().index(pos_label)]\n",
    "    else:\n",
    "        return pred_probs\n",
    "\n",
    "# --- Main Inference Logic ---\n",
    "if spark_session:\n",
    "    # --- Data Loading ---\n",
    "    df_processed_dataset = dataio.from_delta_table(\n",
    "        f\"{catalog}.{read_schema}.{sst_job_id}_processed_dataset\",\n",
    "        spark_session=spark_session,\n",
    "    )\n",
    "    unique_ids = df_processed_dataset[cfg.student_id_col]\n",
    "    df_train = modeling.evaluation.extract_training_data_from_model(experiment_id)\n",
    "\n",
    "    # --- Model Loading ---\n",
    "    loaded_model = mlflow_load_model(model_uri, model_type)\n",
    "\n",
    "    # --- Inference Parameters ---\n",
    "    inference_params = {\n",
    "        \"num_top_features\": 5,\n",
    "        \"min_prob_pos_label\": 0.5,\n",
    "    }\n",
    "\n",
    "    # --- Feature Selection ---\n",
    "    try:\n",
    "        model_feature_names = loaded_model.named_steps[\"column_selector\"].get_params()[\n",
    "            \"cols\"\n",
    "        ]\n",
    "    except AttributeError:\n",
    "        model_feature_names = loaded_model.metadata.get_input_schema().input_names()\n",
    "    df_serving_dataset = df_processed_dataset[model_feature_names]\n",
    "\n",
    "    # --- Write Inference Dataset ---\n",
    "    inference_dataset_path = f\"{catalog}.{write_schema}.{sst_job_id}_inference_dataset\"\n",
    "    dataio.to_delta_table(\n",
    "        df_serving_dataset, inference_dataset_path, spark_session=spark_session\n",
    "    )\n",
    "\n",
    "    # --- Prediction ---\n",
    "    df_predicted_dataset = df_serving_dataset.copy()\n",
    "    df_predicted_dataset[\"predicted_label\"] = loaded_model.predict(df_serving_dataset)\n",
    "    try:\n",
    "        df_predicted_dataset[\"predicted_prob\"] = loaded_model.predict_proba(\n",
    "            df_serving_dataset\n",
    "        )[:, 1]\n",
    "    except AttributeError:\n",
    "        logging.error(\n",
    "            \"Model does not have predict_proba method. Skipping probability prediction.\"\n",
    "        )\n",
    "\n",
    "    # --- Write Predicted Dataset ---\n",
    "    predicted_dataset_path = f\"{catalog}.{write_schema}.{sst_job_id}_predicted_dataset\"\n",
    "    dataio.to_delta_table(\n",
    "        df_predicted_dataset,\n",
    "        predicted_dataset_path,\n",
    "        spark_session=spark_session,\n",
    "    )\n",
    "    logging.info(\"Predictions saved to: %s\", predicted_dataset_path)\n",
    "\n",
    "    # --- Email notify users ---\n",
    "    # Uncomment below once we want to enable CC'ing to DK's email.\n",
    "    # emails.send_inference_kickoff_email(SENDER_EMAIL, [notif_email], [DK_CC_EMAIL], MANDRILL_USERNAME, MANDRILL_PASSWORD)\n",
    "    emails.send_inference_kickoff_email(\n",
    "        SENDER_EMAIL, [notif_email], [], MANDRILL_USERNAME, MANDRILL_PASSWORD\n",
    "    )\n",
    "\n",
    "    # --- SHAP Values Calculation ---\n",
    "    pred_probs = df_predicted_dataset[\"predicted_prob\"]\n",
    "    shap_ref_data_size = 200\n",
    "    train_mode = df_train.mode().iloc\n",
    "    df_ref = (\n",
    "        df_train.sample(\n",
    "            n=min(shap_ref_data_size, df_train.shape[0]),\n",
    "            random_state=cfg.random_state,\n",
    "        )\n",
    "        .fillna(train_mode)\n",
    "        .loc[:, model_feature_names]\n",
    "    )\n",
    "\n",
    "    explainer = shap.explainers.KernelExplainer(\n",
    "        ft.partial(\n",
    "            predict_proba,\n",
    "            model=loaded_model,\n",
    "            feature_names=model_feature_names,\n",
    "            pos_label=cfg.pos_label,\n",
    "        ),\n",
    "        df_ref,\n",
    "        link=\"identity\",\n",
    "    )\n",
    "\n",
    "    # Calculate Shap values using the KernelExplainer\n",
    "    df_shap_values = explainer(df_processed_dataset[model_feature_names])\n",
    "\n",
    "    # --- SHAP Summary Plot ---\n",
    "    # Plot Shap values using the beeswarm plot function\n",
    "    shap_fig = plot_shap_beeswarm(df_shap_values)\n",
    "    \n",
    "    # --- Feature Selection for Display ---\n",
    "    result = inference.select_top_features_for_display(\n",
    "        df_serving_dataset,\n",
    "        unique_ids,\n",
    "        pred_probs,\n",
    "        df_shap_values.values,\n",
    "        n_features=inference_params[\"num_top_features\"],\n",
    "        features_table=features_table,\n",
    "        needs_support_threshold_prob=inference_params[\"min_prob_pos_label\"],\n",
    "    )\n",
    "\n",
    "    # Write the inference-ready dataset to Delta Lake.\n",
    "    shap_results_path = f\"{catalog}.{write_schema}.{sst_job_id}_shap_results_dataset\"\n",
    "    dataio.to_delta_table(result, shap_results_path, spark_session=spark_session)\n",
    "\n",
    "    # --- Save Results to ext/ folder in Gold volume. ---\n",
    "    # Specify where in the gold volume these output files should be stored.\n",
    "    result_path = f\"/Volumes/{DB_workspace}/{institution_name}_gold/gold_volume/inference_jobs/{sst_job_id}/ext/\"\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    # Write the DataFrame to CSV in the specified volume\n",
    "    spark_df = spark_session.createDataFrame(result)\n",
    "    # Note that this writes multiple files under the save() parameter as a directory.\n",
    "    spark_df.coalesce(1).write.format(\"csv\").option(\"header\", \"true\").mode(\n",
    "        \"overwrite\"\n",
    "    ).save(result_path + \"inference_output\")\n",
    "\n",
    "    # Write the SHAP chart png to the volume\n",
    "    shap_fig.savefig(result_path + \"shap_chart.png\", bbox_inches = \"tight\")\n",
    "\n",
    "else:\n",
    "    logging.error(\"Spark session not initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c7d9763-a2a7-45ba-98b7-9baa3776b9cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO there are model dependencies that need to be installed at runtime\n",
    "# This was the error receieved (although it still worked)\n",
    "\"\"\"\n",
    "- mlflow (current: 2.20.0, required: mlflow==2.19.0)\n",
    "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pdp_model_inference",
   "widgets": {
    "DB_workspace": {
     "currentValue": "dev_sst_02",
     "nuid": "cdc996d3-d00e-4715-97c6-e6ccef679222",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "uni_of_crystal_testing",
      "label": "",
      "name": "DB_workspace",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "uni_of_crystal_testing",
      "label": "",
      "name": "DB_workspace",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "databricks_institution_name": {
     "currentValue": "uni_of_crystal_testing",
     "nuid": "00e6f334-1d3c-4223-8c8e-6b7de87934ed",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "",
      "label": "",
      "name": "databricks_institution_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "",
      "label": "",
      "name": "databricks_institution_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "db_run_id": {
     "currentValue": "928391757639049",
     "nuid": "96df5599-835c-43c8-9cdb-03be203177d7",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "1234567",
      "label": "",
      "name": "db_run_id",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "1234567",
      "label": "",
      "name": "db_run_id",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    },
    "model_name": {
     "currentValue": "latest_enrollment_model",
     "nuid": "f8d7a328-8c46-4faf-940e-c00366eeabd8",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "latest_enrollment_model",
      "label": "",
      "name": "model_name",
      "options": {
       "widgetDisplayType": "Text",
       "validationRegex": null
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "widgetType": "text",
      "defaultValue": "latest_enrollment_model",
      "label": "",
      "name": "model_name",
      "options": {
       "widgetType": "text",
       "autoCreated": false,
       "validationRegex": null
      }
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
