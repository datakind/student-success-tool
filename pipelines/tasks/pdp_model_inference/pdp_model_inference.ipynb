{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6107cdd3-68e1-4242-8831-d3c06f40338b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.9.6' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "This script performs model inference for the Student Success Tool (SST) pipeline.\n",
    "\n",
    "It loads a pre-trained ML model from MLflow Model Registry, \n",
    "reads a processed dataset from Delta Lake, performs inference, calculates SHAP values, \n",
    "and writes the predictions back to Delta Lake.  \n",
    "\n",
    "The notebook is designed to run within a Databricks environment as a job task, leveraging Databricks \n",
    "utilities for widget input, job task values, and Spark session management.\n",
    "\n",
    "This is a POC notebook, it is advised to refactor to .py and add tests before using in production.\n",
    "\"\"\"\n",
    "\n",
    "# Import necessary libraries\n",
    "import logging\n",
    "import os\n",
    "\n",
    "import typing as t\n",
    "import functools as ft\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "from databricks.connect import DatabricksSession\n",
    "from databricks.sdk.runtime import dbutils  # noqa: F401\n",
    "from pyspark.sql.types import FloatType, StringType, StructField, StructType\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from email.headerregistry import Address\n",
    "\n",
    "# Import project-specific modules\n",
    "import student_success_tool.dataio as dataio\n",
    "from student_success_tool.modeling import inference\n",
    "import student_success_tool.modeling as modeling\n",
    "from student_success_tool.schemas import pdp as schemas\n",
    "from student_success_tool import emails\n",
    "\n",
    "# Disable mlflow autologging (prevents conflicts in Databricks environments)\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.WARNING)  # Suppress py4j logging\n",
    "\n",
    "# --- Spark Session Initialization ---\n",
    "try:\n",
    "    spark_session = DatabricksSession.builder.getOrCreate()\n",
    "except Exception:\n",
    "    logging.warning(\"Unable to create Spark session; are you in a Databricks runtime?\")\n",
    "    spark_session = None\n",
    "\n",
    "# --- Configuration ---\n",
    "# Input Parameters ( from Databricks widgets)\n",
    "DB_workspace = dbutils.widgets.get(\"DB_workspace\")\n",
    "catalog = DB_workspace\n",
    "institution_name = dbutils.widgets.get(\"databricks_institution_name\")\n",
    "sst_job_id = dbutils.widgets.get(\"db_run_id\")\n",
    "model_name = dbutils.widgets.get(\"model_name\")\n",
    "model_version = dbutils.widgets.get(\"version_id\")\n",
    "model_type = dbutils.widgets.get(\"model_type\")\n",
    "notif_email = dbutils.widgets.get(\"notification_email\")\n",
    "\n",
    "# Secrets from Databricks\n",
    "w = WorkspaceClient()\n",
    "MANDRILL_USERNAME = w.dbutils.secrets.get(scope=\"sst\", key=\"MANDRILL_USERNAME\")\n",
    "MANDRILL_PASSWORD = w.dbutils.secrets.get(scope=\"sst\", key=\"MANDRILL_PASSWORD\")\n",
    "SENDER_EMAIL = Address(\"Datakind Info\", \"help\", \"datakind.org\")\n",
    "DK_CC_EMAIL = 'education@datakind.org'\n",
    "\n",
    "# --- Unity caatalog schemas ---\n",
    "read_schema = f\"{institution_name}_silver\"\n",
    "write_schema = f\"{institution_name}_silver\"\n",
    "model_schema = f\"{institution_name}_gold\"\n",
    "\n",
    "\n",
    "# --- Insititution Configuration ---\n",
    "cfg = dataio.read_config(\n",
    "    f\"/Volumes/{DB_workspace}/{institution_name}_gold/gold_volume/configuration_files/{institution_name}_{model_name}_configuration_file.toml\",\n",
    "    schema=schemas.PDPProjectConfig,\n",
    ")\n",
    "\n",
    "# --- Model Configuration ---\n",
    "# model_run_id = \"890b54cf68b147d7a55f515f61d5bfb2\"\n",
    "# experiment_id = \"1510364684601785\"\n",
    "experiment_id = cfg.models[\"graduation\"].experiment_id\n",
    "model_run_id = cfg.models[\"graduation\"].run_id\n",
    "model_uri = f\"models:/{catalog}.{model_schema}.{model_name}/1\"\n",
    "\n",
    "\n",
    "# --- Load features table ---\n",
    "features_table = dataio.read_features_table(\"assets/pdp/features_table.toml\")\n",
    "\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def mlflow_load_model(model_uri: str, model_type: str):\n",
    "    \"\"\"Loads an MLflow model based on its type.\"\"\"\n",
    "\n",
    "    # Dictionary mapping model types to loading functions\n",
    "    load_model_func = {\n",
    "        \"sklearn\": mlflow.sklearn.load_model,\n",
    "        \"xgboost\": mlflow.xgboost.load_model,\n",
    "        \"lightgbm\": mlflow.lightgbm.load_model,\n",
    "        \"pyfunc\": mlflow.pyfunc.load_model,  # Default\n",
    "    }.get(model_type, mlflow.pyfunc.load_model)\n",
    "\n",
    "    model = load_model_func(model_uri)\n",
    "    logging.info(\"MLflow '%s' model loaded from '%s'\", model_type, model_uri)\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_proba(\n",
    "    X,\n",
    "    model,\n",
    "    *,\n",
    "    feature_names: t.Optional[list[str]] = None,\n",
    "    pos_label: t.Optional[bool | str] = None,\n",
    ") -> np.ndarray:\n",
    "    \"\"\"Predicts probabilities using the provided model.\"\"\"\n",
    "\n",
    "    if feature_names is None:\n",
    "        feature_names = model.named_steps[\"column_selector\"].get_params()[\"cols\"]\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(data=X, columns=feature_names)\n",
    "    else:\n",
    "        assert X.shape == len(feature_names)\n",
    "    pred_probs = model.predict_proba(X)\n",
    "    if pos_label is not None:\n",
    "        return pred_probs[:, model.classes_.tolist().index(pos_label)]\n",
    "    else:\n",
    "        return pred_probs\n",
    "\n",
    "\n",
    "# --- Main Inference Logic ---\n",
    "if spark_session:\n",
    "    # --- Data Loading ---\n",
    "    df_processed_dataset = dataio.from_delta_table(\n",
    "        f\"{catalog}.{read_schema}.{sst_job_id}_processed_dataset\",\n",
    "        spark_session=spark_session,\n",
    "    )\n",
    "    unique_ids = df_processed_dataset[cfg.student_id_col]\n",
    "    df_train = modeling.evaluation.extract_training_data_from_model(experiment_id)\n",
    "\n",
    "    # --- Model Loading ---\n",
    "    loaded_model = mlflow_load_model(model_uri, model_type)\n",
    "\n",
    "    # --- Inference Parameters ---\n",
    "    inference_params = {\n",
    "        \"num_top_features\": 5,\n",
    "        \"min_prob_pos_label\": 0.5,\n",
    "    }\n",
    "\n",
    "    # --- Feature Selection ---\n",
    "    try:\n",
    "        model_feature_names = loaded_model.named_steps[\"column_selector\"].get_params()[\n",
    "            \"cols\"\n",
    "        ]\n",
    "    except AttributeError:\n",
    "        model_feature_names = loaded_model.metadata.get_input_schema().input_names()\n",
    "    df_serving_dataset = df_processed_dataset[model_feature_names]\n",
    "\n",
    "    # --- Write Inference Dataset ---\n",
    "    inference_dataset_path = f\"{catalog}.{write_schema}.{sst_job_id}_inference_dataset\"\n",
    "    dataio.to_delta_table(\n",
    "        df_serving_dataset, inference_dataset_path, spark_session=spark_session\n",
    "    )\n",
    "\n",
    "    # --- Prediction ---\n",
    "    df_predicted_dataset = df_serving_dataset.copy()\n",
    "    df_predicted_dataset[\"predicted_label\"] = loaded_model.predict(df_serving_dataset)\n",
    "    try:\n",
    "        df_predicted_dataset[\"predicted_prob\"] = loaded_model.predict_proba(\n",
    "            df_serving_dataset\n",
    "        )[:, 1]\n",
    "    except AttributeError:\n",
    "        logging.error(\n",
    "            \"Model does not have predict_proba method. Skipping probability prediction.\"\n",
    "        )\n",
    "\n",
    "    # --- Write Predicted Dataset ---\n",
    "    predicted_dataset_path = f\"{catalog}.{write_schema}.{sst_job_id}_predicted_dataset\"\n",
    "    dataio.to_delta_table(\n",
    "        df_predicted_dataset,\n",
    "        predicted_dataset_path,\n",
    "        spark_session=spark_session,\n",
    "    )\n",
    "    logging.info(\"Predictions saved to: %s\",predicted_dataset_path)\n",
    "\n",
    "\n",
    "    # --- Email notify users ---\n",
    "    # Uncomment below once we want to enable CC'ing to DK's email.\n",
    "    # emails.send_inference_kickoff_email(SENDER_EMAIL, [notif_email], [DK_CC_EMAIL], MANDRILL_USERNAME, MANDRILL_PASSWORD)\n",
    "    emails.send_inference_kickoff_email(SENDER_EMAIL, [notif_email], [], MANDRILL_USERNAME, MANDRILL_PASSWORD)\n",
    "\n",
    "    # --- SHAP Values Calculation ---\n",
    "    pred_probs = df_predicted_dataset[\"predicted_prob\"]\n",
    "\n",
    "    # TODO: Consider saving the explainer during training\n",
    "    # TODO: Pedro's note: Consider getting shap_ref_data_size from a workflow parameter or from the toml config file\n",
    "    shap_ref_data_size = 200\n",
    "    train_mode = df_train.mode().iloc\n",
    "    df_ref = (\n",
    "        df_train.sample(\n",
    "            n=min(shap_ref_data_size, df_train.shape[0]),\n",
    "            random_state=cfg.random_state,\n",
    "        )\n",
    "        .fillna(train_mode)\n",
    "        .loc[:, model_feature_names]\n",
    "    )\n",
    "\n",
    "    explainer = shap.explainers.KernelExplainer(\n",
    "        ft.partial(\n",
    "            predict_proba,\n",
    "            model=loaded_model,\n",
    "            feature_names=model_feature_names,\n",
    "            pos_label=cfg.pos_label,\n",
    "        ),\n",
    "        df_ref,\n",
    "        link=\"identity\",\n",
    "    )\n",
    "\n",
    "    shap_schema = StructType(\n",
    "        [StructField(cfg.student_id_col, StringType(), nullable=False)]\n",
    "        + [StructField(col, FloatType(), nullable=False) for col in model_feature_names]\n",
    "    )\n",
    "\n",
    "    df_shap_values = (\n",
    "        spark_session.createDataFrame(\n",
    "            df_processed_dataset.reindex(\n",
    "                columns=model_feature_names + [cfg.student_id_col]\n",
    "            )\n",
    "        )\n",
    "        .repartition(spark_session.sparkContext.defaultParallelism)\n",
    "        .mapInPandas(\n",
    "            ft.partial(\n",
    "                inference.calculate_shap_values_spark_udf,\n",
    "                student_id_col=cfg.student_id_col,\n",
    "                model_features=model_feature_names,\n",
    "                explainer=explainer,\n",
    "                mode=train_mode,\n",
    "            ),\n",
    "            schema=shap_schema,\n",
    "        )\n",
    "        .toPandas()\n",
    "        .set_index(cfg.student_id_col)\n",
    "        .reindex(df_processed_dataset[cfg.student_id_col])\n",
    "        .reset_index(drop=False)\n",
    "    )\n",
    "\n",
    "    # --- SHAP Summary Plot ---\n",
    "    # NOTE: to change colors https://stackoverflow.com/questions/60153036/changing-the-gradient-color-of-shap-summary-plot-to-specific-2-or-3-rgb-grad\n",
    "    shap.summary_plot(\n",
    "        df_shap_values.loc[:, model_feature_names].to_numpy(),\n",
    "        df_serving_dataset.loc[:, model_feature_names],\n",
    "        class_names=loaded_model.classes_,\n",
    "        max_display=20,\n",
    "        show=False,\n",
    "    )\n",
    "    shap_fig = plt.gcf()\n",
    "\n",
    "    # # --- Log SHAP Plot to MLflow ---\n",
    "    # with mlflow.start_run(run_id=model_run_id) as run:\n",
    "    #     mlflow.log_figure(\n",
    "    #         shap_fig,\n",
    "    #         f\"shap_summary_dataset_name_dataset_\"\n",
    "    #         f\"{df_ref.shape}_ref_rows.png\",\n",
    "    #     )\n",
    "\n",
    "    # --- Feature Selection for Display ---\n",
    "    result = inference.select_top_features_for_display(\n",
    "        df_serving_dataset,\n",
    "        unique_ids,\n",
    "        pred_probs,\n",
    "        df_shap_values[model_feature_names].to_numpy(),\n",
    "        n_features=inference_params[\"num_top_features\"],\n",
    "        features_table=features_table,\n",
    "        needs_support_threshold_prob=inference_params[\"min_prob_pos_label\"],\n",
    "    )\n",
    "    # Write the inference-ready dataset to Delta Lake.\n",
    "    shap_results_path = f\"{catalog}.{write_schema}.{sst_job_id}_shap_results_dataset\"\n",
    "    dataio.to_delta_table(result, shap_results_path, spark_session=spark_session)\n",
    "\n",
    "    # --- Save Results to ext/ folder in Gold volume. ---\n",
    "    # Specify where in the gold volume these output files should be stored.\n",
    "    result_path = f\"/Volumes/{DB_workspace}/{institution_name}_gold/gold_volume/inference_jobs/{sst_job_id}/ext/\"\n",
    "    os.makedirs(result_path, exist_ok=True)\n",
    "\n",
    "    # Write the DataFrame to CSV in the specified volume\n",
    "    spark_df = spark.createDataFrame(result)\n",
    "    # Note that this writes multiple files under the save() parameter as a directory.\n",
    "    spark_df.coalesce(1).write.format(\"csv\").option(\"header\", \"true\").mode(\"overwrite\").save(result_path+'inference_output')\n",
    "\n",
    "    # Write the SHAP chart png to the volume\n",
    "    shap_fig.savefig(result_path+'shap_chart.png')\n",
    "\n",
    "else:\n",
    "    logging.error(\"Spark session not initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c7d9763-a2a7-45ba-98b7-9baa3776b9cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO there are model dependencies that need to be installed at runtime\n",
    "# This was the error receieved (although it still worked)\n",
    "\"\"\"\n",
    "- mlflow (current: 2.20.0, required: mlflow==2.19.0)\n",
    "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a602528a-c898-4d00-9331-dadcef24ceb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO save the charts\n",
    "# # # --- Log SHAP Plot to MLflow ---\n",
    "# with mlflow.start_run(run_id=model_run_id) as run:\n",
    "#     mlflow.log_figure(\n",
    "#         shap_fig,\n",
    "#         f\"shap_summary_dataset_name_dataset_\"\n",
    "#         f\"{df_ref.shape}_{sst_job_id}_shap_results.png\",\n",
    "#     )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pdp_model_inference",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
