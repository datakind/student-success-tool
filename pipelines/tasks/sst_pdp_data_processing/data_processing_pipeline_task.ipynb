{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5c5d41a-57e2-4b2c-a1fb-129131bef244",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script prepares data for inference in the Student Success Tool (SST) pipeline.\n",
    "\n",
    "It reads validated course and cohort data from Delta Lake tables, creates a student-term \n",
    "dataset, applies target variable logic (currently using a workaround due to library \n",
    "limitations), and saves the processed dataset back to a Delta Lake table.  It's \n",
    "designed to run within a Databricks environment.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "\n",
    "import mlflow\n",
    "from databricks.connect import DatabricksSession\n",
    "from databricks.sdk.runtime import dbutils\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "import student_success_tool.dataio as dataio\n",
    "import student_success_tool.targets.pdp as targets\n",
    "import student_success_tool.schemas.pdp as schemas\n",
    "import student_success_tool.preprocessing.pdp as preprocessing\n",
    "\n",
    "\n",
    "# Disable mlflow autologging (due to Databricks issues during feature selection)\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.WARNING)  # Ignore Databricks logger\n",
    "\n",
    "# Attempt to create a Spark session\n",
    "try:\n",
    "    spark_session = DatabricksSession.builder.getOrCreate()\n",
    "except Exception:\n",
    "    logging.warning(\"Unable to create Spark session; are you in a Databricks runtime?\")\n",
    "    spark_session = None\n",
    "\n",
    "# Databricks workspace identifier\n",
    "DB_workspace = dbutils.widgets.get(\"DB_workspace\")\n",
    "model_name = dbutils.widgets.get(\"model_name\")\n",
    "\n",
    "# Input parameters from Databricks widgets\n",
    "institution_id = dbutils.widgets.get(\"institution_id\")\n",
    "sst_job_id = dbutils.widgets.get(\"sst_job_id\")\n",
    "\n",
    "# Delta Lake table details (read from job task values set by data ingestion task)\n",
    "catalog = DB_workspace\n",
    "read_schema = f\"{institution_id}_bronze\"\n",
    "write_schema = f\"{institution_id}_silver\"\n",
    "\n",
    "# Read DataFrames from Delta Lake tables (if Spark session is available)\n",
    "if spark_session:\n",
    "    df_course = schemas.RawPDPCourseDataSchema(\n",
    "        dataio.from_delta_table(\n",
    "            f\"{catalog}.{read_schema}.{sst_job_id}_course_dataset_validated\",\n",
    "            spark_session=spark_session,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_cohort = schemas.RawPDPCohortDataSchema(\n",
    "        dataio.from_delta_table(\n",
    "            f\"{catalog}.{read_schema}.{sst_job_id}_cohort_dataset_validated\",\n",
    "            spark_session=spark_session,\n",
    "        )\n",
    "    )\n",
    "else:\n",
    "    logging.warning(\"Spark session not initialized. Cannot read dataframes.\")\n",
    "    exit()  # Exit the script if the Spark session is not available.\n",
    "\n",
    "\n",
    "# Reading the parameters from the institution's configuration file\n",
    "cfg = dataio.read_config(\n",
    "    f\"/Volumes/{DB_workspace}/{institution_id}_gold/gold_volume/configuration_files/{institution_id}_{model_name}_configuration_file.toml\",\n",
    "    schema=schemas.PDPProjectConfig,\n",
    ")\n",
    "\n",
    "# Read preprocessing features\n",
    "min_passing_grade = cfg.preprocessing.features.min_passing_grade\n",
    "min_num_credits_full_time = cfg.preprocessing.features.min_num_credits_full_time\n",
    "course_level_pattern = cfg.preprocessing.features.course_level_pattern\n",
    "key_course_subject_areas = cfg.preprocessing.features.key_course_subject_areas\n",
    "key_course_ids = cfg.preprocessing.features.key_course_ids\n",
    "\n",
    "# Read preprocessing target params\n",
    "min_num_credits_checkin = cfg.preprocessing.target.params[\"min_num_credits_checkin\"]\n",
    "min_num_credits_target = cfg.preprocessing.target.params[\"min_num_credits_target\"]\n",
    "student_criteria = cfg.preprocessing.target.student_criteria\n",
    "student_id_col = cfg.student_id_col\n",
    "\n",
    "# Create student-term dataset\n",
    "df_student_terms = preprocessing.dataops.make_student_term_dataset(\n",
    "    df_cohort,\n",
    "    df_course,\n",
    "    min_passing_grade=min_passing_grade,\n",
    "    min_num_credits_full_time=min_num_credits_full_time,\n",
    "    course_level_pattern=course_level_pattern,\n",
    "    key_course_subject_areas=key_course_subject_areas,\n",
    "    key_course_ids=key_course_ids,\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "eligible_students = targets.shared.select_students_by_criteria(\n",
    "    df_student_terms,\n",
    "    student_id_cols=student_id_col,\n",
    "    **student_criteria\n",
    ")\n",
    "max_term_rank = df_student_terms[\"term_rank\"].max()\n",
    "\n",
    "df_processed = pd.merge(\n",
    "    df_student_terms.loc[df_student_terms[\"term_rank\"].eq(max_term_rank), :],\n",
    "    eligible_students,\n",
    "    on=student_id_col,\n",
    "    how=\"inner\",\n",
    ")\n",
    "df_processed = preprocessing.dataops.clean_up_labeled_dataset_cols_and_vals(\n",
    "    df_processed\n",
    ")\n",
    "\n",
    "# Save processed dataset to Delta Lake (if Spark session is available)\n",
    "if spark_session:\n",
    "    write_table_path = f\"{catalog}.{write_schema}.{sst_job_id}_processed_dataset\"\n",
    "    dataio.to_delta_table(df_processed, write_table_path, spark_session=spark_session)\n",
    "    logging.info(f\"Processed dataset written to: {write_table_path}\")\n",
    "else:\n",
    "    logging.warning(\"Spark session not initialized. Cannot write processed dataset.\")\n",
    "\n",
    "dbutils.jobs.taskValues.set(key=\"processed_dataset_path\", value=write_table_path)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "data_processing_pipeline_task",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
