{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf9ebb0f-7638-4b96-b56d-488f223b8bd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This notebook ingests course and cohort data for the Student Success Tool (SST) pipeline.\n",
    "\n",
    "It reads data from CSV files stored in a Google Cloud Storage (GCS) bucket, \n",
    "performs schema validation using the `pdp` library, and writes the validated data \n",
    "to Delta Lake tables in Databricks Unity Catalog.\n",
    "\n",
    "The notebook is designed to run within a Databricks environment as a job task, leveraging Databricks \n",
    "utilities for widget input, job task values, and Spark session management.\n",
    "\n",
    "This is a POC notebook, it is advised to refactor to .py and add tests before using in production.\n",
    "\n",
    "\"\"\"\n",
    "import logging\n",
    "import os\n",
    "\n",
    "from databricks.connect import DatabricksSession\n",
    "from databricks.sdk import WorkspaceClient\n",
    "from databricks.sdk.runtime import dbutils\n",
    "from google.cloud import storage\n",
    "from email.headerregistry import Address\n",
    "\n",
    "import student_success_tool.dataio as dataio\n",
    "from student_success_tool.schemas import pdp as schemas\n",
    "from student_success_tool import emails\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.WARNING)  # Ignore Databricks logger\n",
    "\n",
    "# Attempt to create a Spark session. Handles exceptions if not in Databricks.\n",
    "try:\n",
    "    spark_session = DatabricksSession.builder.getOrCreate()\n",
    "except Exception:\n",
    "    logging.warning(\"Unable to create Spark session; are you in a Databricks runtime?\")\n",
    "    spark_session = None\n",
    "\n",
    "# Input parameters (provided via Databricks widgets or job task values)\n",
    "DB_workspace = dbutils.widgets.get(\"DB_workspace\")  # Databricks workspace identifier\n",
    "notif_email = dbutils.widgets.get(\"notification_email\")\n",
    "# The institution_name parameter is the databricksified institution name and is NOT the same as the institution id in GCP.\n",
    "institution_name = dbutils.widgets.get(\"databricks_institution_name\")\n",
    "course_file_name = dbutils.widgets.get(\"course_file_name\")\n",
    "cohort_file_name = dbutils.widgets.get(\"cohort_file_name\")\n",
    "# FYI: the job run id corresponds to the {{parent_run_id}} for jobs with two or more tasks https://stackoverflow.com/questions/75428900/confusion-about-run-id-and-parent-run-id-variables-for-databricks-jobs\n",
    "db_run_id = dbutils.widgets.get(\"db_run_id\")\n",
    "# This is the name of the external bucket -- the internal bucket is simply this name with _internal suffixed.\n",
    "gcp_bucket_name = dbutils.widgets.get(\"gcp_bucket_name\")\n",
    "\n",
    "# Notify user that an inference run has been kicked off.\n",
    "w = WorkspaceClient()\n",
    "MANDRILL_USERNAME = w.dbutils.secrets.get(scope=\"sst\", key=\"MANDRILL_USERNAME\")\n",
    "MANDRILL_PASSWORD = w.dbutils.secrets.get(scope=\"sst\", key=\"MANDRILL_PASSWORD\")\n",
    "SENDER_EMAIL = Address(\"Datakind Info\", \"help\", \"datakind.org\")\n",
    "DK_CC_EMAIL = \"education@datakind.org\"\n",
    "# NOT WORKING -- can't find the package\n",
    "# emails.send_inference_kickoff_email(SENDER_EMAIL, [notif_email], [], MANDRILL_USERNAME, MANDRILL_PASSWORD)\n",
    "\n",
    "# Define paths (using Unity Catalog volumes)\n",
    "internal_pipeline_path = f\"/Volumes/{DB_workspace}/{institution_name}_bronze/bronze_volume/inference_jobs/{db_run_id}/raw_files/\"\n",
    "\n",
    "\n",
    "# Create internal pipeline directory\n",
    "os.makedirs(internal_pipeline_path, exist_ok=True)\n",
    "\n",
    "# Initialize GCS client\n",
    "storage_client = storage.Client()\n",
    "bucket = storage_client.bucket(gcp_bucket_name)\n",
    "sst_container_folder = \"validated\"\n",
    "\n",
    "# Download course data from GCS\n",
    "course_blob_name = f\"{sst_container_folder}/{course_file_name}\"\n",
    "course_blob = bucket.blob(course_blob_name)\n",
    "course_blob.download_to_filename(f\"{internal_pipeline_path}{course_file_name}\")\n",
    "\n",
    "# Download cohort data from GCS\n",
    "cohort_blob_name = f\"{sst_container_folder}/{cohort_file_name}\"\n",
    "cohort_blob = bucket.blob(cohort_blob_name)\n",
    "cohort_blob.download_to_filename(f\"{internal_pipeline_path}{cohort_file_name}\")\n",
    "\n",
    "\n",
    "# Set path_volume (important for compatibility with Datakind's code)\n",
    "path_volume = internal_pipeline_path\n",
    "\n",
    "# Construct full file paths\n",
    "fpath_course = os.path.join(path_volume, course_file_name)\n",
    "fpath_cohort = os.path.join(path_volume, cohort_file_name)\n",
    "\n",
    "# Read data from CSV files into Pandas DataFrames and validate schema\n",
    "df_course = dataio.pdp.read_raw_course_data(\n",
    "    file_path=fpath_course,\n",
    "    schema=schemas.RawPDPCourseDataSchema,\n",
    "    dttm_format=\"%Y-%m-%d\",\n",
    ")\n",
    "df_cohort = dataio.pdp.read_raw_cohort_data(\n",
    "    file_path=fpath_cohort, schema=schemas.RawPDPCohortDataSchema\n",
    ")\n",
    "\n",
    "\n",
    "# Define Delta Lake table details\n",
    "catalog = DB_workspace\n",
    "write_schema = f\"{institution_name}_bronze\"\n",
    "\n",
    "\n",
    "# Write DataFrames to Delta Lake tables (only if Spark session is available)\n",
    "if spark_session:\n",
    "    dataio.to_delta_table(\n",
    "        df_course,\n",
    "        f\"{catalog}.{write_schema}.{db_run_id}_course_dataset_validated\",\n",
    "        spark_session=spark_session,\n",
    "    )\n",
    "\n",
    "    dataio.to_delta_table(\n",
    "        df_cohort,\n",
    "        f\"{catalog}.{write_schema}.{db_run_id}_cohort_dataset_validated\",\n",
    "        spark_session=spark_session,\n",
    "    )\n",
    "\n",
    "    # Verify Delta Lake write by reading data back\n",
    "    df_course_from_catalog = schemas.RawPDPCourseDataSchema(\n",
    "        dataio.from_delta_table(\n",
    "            f\"{catalog}.{write_schema}.{db_run_id}_course_dataset_validated\",\n",
    "            spark_session=spark_session,\n",
    "        )\n",
    "    )\n",
    "    print(f\"Course DataFrame shape from catalog: {df_course_from_catalog.shape}\")\n",
    "\n",
    "    df_cohort_from_catalog = schemas.RawPDPCohortDataSchema(\n",
    "        dataio.from_delta_table(\n",
    "            f\"{catalog}.{write_schema}.{db_run_id}_cohort_dataset_validated\",\n",
    "            spark_session=spark_session,\n",
    "        )\n",
    "    )\n",
    "    print(f\"Cohort DataFrame shape from catalog: {df_cohort_from_catalog.shape}\")\n",
    "else:\n",
    "    logging.warning(\n",
    "        \"Spark session not initialized. Skipping Delta Lake write and verification.\"\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "pdp_data_ingestion",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
