{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "839ac7da-09c0-4d9c-ab75-ccb0ed0cb21d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script performs model inference for the Student Success Tool (SST) pipeline.\n",
    "\n",
    "It loads a pre-trained ML model from MLflow Model Registry, reads a processed \n",
    "dataset from Delta Lake, performs inference, and writes the predictions back to \n",
    "Delta Lake.  It's designed to run within a Databricks environment.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "\n",
    "import mlflow\n",
    "from databricks.connect import DatabricksSession\n",
    "from databricks.sdk.runtime import dbutils\n",
    "\n",
    "import student_success_tool.dataio as dataio\n",
    "\n",
    "\n",
    "# Disable mlflow autologging (prevents conflicts in Databricks environments).\n",
    "mlflow.autolog(disable=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.WARNING)  # Suppress py4j logging.\n",
    "\n",
    "# Attempt to create a Spark session. Handles non-Databricks environments.\n",
    "try:\n",
    "    spark_session = DatabricksSession.builder.getOrCreate()\n",
    "except Exception:\n",
    "    logging.warning(\"Unable to create Spark session; are you in a Databricks runtime?\")\n",
    "    spark_session = None\n",
    "\n",
    "# --- Input Parameters (from Databricks widgets) ---\n",
    "DB_workspace = dbutils.widgets.get(\"DB_workspace\")  # Databricks workspace identifier.\n",
    "\n",
    "\n",
    "institution_id = dbutils.widgets.get(\"institution_id\")\n",
    "model_name = dbutils.widgets.get(\"model_name\")\n",
    "sst_job_id = dbutils.widgets.get(\"sst_job_id\")\n",
    "model_type = dbutils.widgets.get(\"model_type\")\n",
    "# TODO: Determine how to track and pass the model URI.\n",
    "\n",
    "# --- Delta Lake Configuration ---\n",
    "catalog = DB_workspace\n",
    "# write_schema = dbutils.jobs.taskValues.get(taskKey=\"data_ingestion\", key=\"write_schema\")  # From data ingestion task.\n",
    "read_schema = f\"{institution_id}_silver\"\n",
    "write_schema = f\"{institution_id}_silver\"\n",
    "model_schema = f\"{institution_id}_gold\"\n",
    "\n",
    "model_uri = f\"models:/{catalog}.{model_schema}.{model_name}/1\"  # Construct model URI.\n",
    "\n",
    "\n",
    "# --- Model Loading ---\n",
    "def mlflow_load_model(model_uri: str, model_type: str):\n",
    "    \"\"\"Loads an MLflow model based on its type.\"\"\"\n",
    "\n",
    "    load_model_func = {  # Dictionary to map model types to loading functions.\n",
    "        \"sklearn\": mlflow.sklearn.load_model,\n",
    "        \"xgboost\": mlflow.xgboost.load_model,\n",
    "        \"lightgbm\": mlflow.lightgbm.load_model,\n",
    "        \"pyfunc\": mlflow.pyfunc.load_model,  # Default if type is not recognized\n",
    "    }.get(\n",
    "        model_type, mlflow.pyfunc.load_model\n",
    "    )  # Default to pyfunc if model_type not found.\n",
    "\n",
    "    model = load_model_func(model_uri)\n",
    "    logging.info(f\"MLflow '{model_type}' model loaded from '{model_uri}'\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Load the specified model.\n",
    "loaded_model = mlflow_load_model(model_uri, model_type)\n",
    "\n",
    "# --- Data Loading, Inference, and Saving ---\n",
    "if spark_session:\n",
    "    # Load the dataset prepared for inference.\n",
    "    df_serving_dataset = dataio.from_delta_table(\n",
    "        f\"{catalog}.{read_schema}.{sst_job_id}_processed_dataset\",\n",
    "        spark_session=spark_session,\n",
    "    )\n",
    "\n",
    "    # Ensure the input data matches the model's expected input schema.\n",
    "    try:\n",
    "        model_columns = loaded_model.named_steps[\"column_selector\"].get_params()[\"cols\"]\n",
    "    except (\n",
    "        AttributeError\n",
    "    ):  # if the model doesn't have a column_selector, it is likely a pyfunc model\n",
    "        model_columns = loaded_model.metadata.get_input_schema().input_names()\n",
    "    df_serving_dataset = df_serving_dataset[model_columns]\n",
    "\n",
    "    # Write the inference-ready dataset back to Delta Lake.\n",
    "    inference_dataset_path = f\"{catalog}.{write_schema}.{sst_job_id}_inference_dataset\"\n",
    "    dataio.to_delta_table(\n",
    "        df_serving_dataset, inference_dataset_path, spark_session=spark_session\n",
    "    )\n",
    "\n",
    "    # Reload the inference dataset (important for schema consistency with the model input).\n",
    "    df_serving_dataset = dataio.from_delta_table(\n",
    "        inference_dataset_path, spark_session=spark_session\n",
    "    )\n",
    "\n",
    "    # Perform inference.\n",
    "    df_serving_dataset[\"predicted_label\"] = loaded_model.predict(df_serving_dataset)\n",
    "\n",
    "    try:\n",
    "        df_serving_dataset[\"predicted_prob\"] = loaded_model.predict_proba(\n",
    "            df_serving_dataset\n",
    "        )[:, 1]\n",
    "    except (\n",
    "        AttributeError\n",
    "    ):  # if the model doesn't have predict_proba, it is likely a pyfunc model\n",
    "        logging.warning(\n",
    "            \"Model does not have predict_proba method. Skipping probability prediction.\"\n",
    "        )\n",
    "        pass\n",
    "\n",
    "    # Write the dataset with predictions to Delta Lake.\n",
    "    predicted_dataset_path = f\"{catalog}.{write_schema}.{sst_job_id}_predicted_dataset\"\n",
    "    dataio.to_delta_table(\n",
    "        df_serving_dataset, predicted_dataset_path, spark_session=spark_session\n",
    "    )\n",
    "    logging.info(f\"Predictions saved to: {predicted_dataset_path}\")\n",
    "\n",
    "    # TODO Shap values\n",
    "\n",
    "else:\n",
    "    logging.warning(\n",
    "        \"Spark session not initialized. Skipping data processing and inference.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5c7d9763-a2a7-45ba-98b7-9baa3776b9cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# TODO there are model dependencies that need to be installed at runtime\n",
    "# This was the error receieved (although it still worked)\n",
    "\"\"\"\n",
    "- mlflow (current: 2.20.0, required: mlflow==2.19.0)\n",
    "To fix the mismatches, call `mlflow.pyfunc.get_model_dependencies(model_uri)` to fetch the model's environment and install dependencies using the resulting environment file.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "model_inference_pipeline_task",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
