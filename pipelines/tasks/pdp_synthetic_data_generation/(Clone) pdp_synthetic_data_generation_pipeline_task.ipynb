{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf9ebb0f-7638-4b96-b56d-488f223b8bd9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "This script generates synthetic student cohort and course data using the faker \n",
    "and pdp libraries, and uploads the generated CSV files to a Google Cloud Storage (GCS) bucket.  \n",
    "\n",
    "The notebook is designed to run within a Databricks environment as a job task, leveraging Databricks \n",
    "utilities for widget input, job task values, and Spark session management.\n",
    "\n",
    "This is a POC notebook, it is advised to refactor to .py and add tests before using in production.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "import faker\n",
    "import pandas as pd\n",
    "\n",
    "from databricks.sdk.runtime import dbutils\n",
    "from google.cloud import storage\n",
    "\n",
    "from student_success_tool.generation import pdp\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logging.getLogger(\"py4j\").setLevel(logging.WARNING)  # Ignore Databricks logger\n",
    "\n",
    "# Databricks workspace identifier\n",
    "DB_workspace = dbutils.widgets.get(\"DB_workspace\")\n",
    "\n",
    "# Input parameters from Databricks widgets\n",
    "institution_id = dbutils.widgets.get(\"databricks_institution_name\")\n",
    "normalize_col_names = (\n",
    "    None\n",
    "    if dbutils.widgets.get(\"normalize_col_names\") == \"\"\n",
    "    else dbutils.widgets.get(\"normalize_col_names\")\n",
    ")\n",
    "avg_num_courses_per_student = int(dbutils.widgets.get(\"avg_num_courses_per_student\"))\n",
    "num_students = int(dbutils.widgets.get(\"num_students\"))\n",
    "\n",
    "\n",
    "# Optional seed for Faker (for reproducibility)\n",
    "seed = None if dbutils.widgets.get(\"seed\") == \"\" else int(dbutils.widgets.get(\"seed\"))\n",
    "\n",
    "sst_job_id = dbutils.widgets.get(\"db_run_id\")\n",
    "\n",
    "# Define the save directory, defaulting to a location within Unity Catalog volumes\n",
    "\n",
    "bucket_name = dbutils.widgets.get(\"gcp_bucket_name\")\n",
    "\n",
    "logging.info(\"Save directory: %s\", bucket_name)\n",
    "logging.info(\"normalize_col_names: %s\", normalize_col_names)\n",
    "logging.info(\"seed: %s\", seed)\n",
    "logging.info(\"num_students: %s\", num_students)\n",
    "\n",
    "\n",
    "# Initialize Faker with optional seed and custom providers\n",
    "faker.Faker.seed(seed)\n",
    "FAKER = faker.Faker()\n",
    "FAKER.add_provider(pdp.raw_cohort.Provider)\n",
    "FAKER.add_provider(pdp.raw_course.Provider)\n",
    "\n",
    "# Initialize GCS client\n",
    "client = storage.Client()\n",
    "\n",
    "\n",
    "bucket = client.bucket(bucket_name)\n",
    "\n",
    "# Generate cohort records\n",
    "cohort_records = [\n",
    "    FAKER.raw_cohort_record(\n",
    "        normalize_col_names=normalize_col_names, institution_id=institution_id\n",
    "    )\n",
    "    for _ in range(num_students)\n",
    "]\n",
    "\n",
    "# Generate course records (related to cohort records)\n",
    "course_records = [\n",
    "    FAKER.raw_course_record(cohort_record, normalize_col_names=normalize_col_names)\n",
    "    for cohort_record in cohort_records\n",
    "    for _ in range(\n",
    "        FAKER.randomize_nb_elements(\n",
    "            avg_num_courses_per_student, min=1\n",
    "        )  # Random number of courses per student\n",
    "    )\n",
    "]\n",
    "\n",
    "# Create Pandas DataFrames\n",
    "df_cohort = pd.DataFrame(cohort_records)\n",
    "df_course = pd.DataFrame(course_records)\n",
    "\n",
    "logging.info(\n",
    "    \"Generated %s cohort records and %s course records\",\n",
    "    len(cohort_records),\n",
    "    len(course_records),\n",
    ")\n",
    "\n",
    "# Construct file names\n",
    "cohort_file_name = f\"{institution_id}_{sst_job_id}_STUDENT_SEMESTER_AR_DEIDENTIFIED.csv\"\n",
    "course_file_name = f\"{institution_id}_{sst_job_id}_COURSE_LEVEL_AR_DEID.csv\"\n",
    "\n",
    "# Construct full paths within the GCS bucket\n",
    "cohort_full_path_name = f\"validated/{cohort_file_name}\"\n",
    "course_full_path_name = f\"validated/{course_file_name}\"\n",
    "\n",
    "# Create GCS blob objects\n",
    "cohort_blob = bucket.blob(cohort_full_path_name)\n",
    "course_blob = bucket.blob(course_full_path_name)\n",
    "\n",
    "# Upload DataFrames to GCS as CSVs\n",
    "cohort_blob.upload_from_string(df_cohort.to_csv(header=True, index=False), \"text/csv\")\n",
    "course_blob.upload_from_string(df_course.to_csv(header=True, index=False), \"text/csv\")\n",
    "\n",
    "print(\n",
    "    f\"Generated and uploaded files: {cohort_file_name} and {course_file_name} to GCS bucket: {bucket_name}\"\n",
    ")\n",
    "\n",
    "# Set Databricks job task values for downstream tasks to access filenames\n",
    "dbutils.jobs.taskValues.set(key=\"cohort_file_name\", value=cohort_file_name)\n",
    "dbutils.jobs.taskValues.set(key=\"course_file_name\", value=course_file_name)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "(Clone) pdp_synthetic_data_generation_pipeline_task",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
