"""Main file for the SST Worker."""

import logging
from typing import Any, Annotated
from fastapi import FastAPI, Depends, HTTPException, status, Security
from fastapi.responses import FileResponse

from pydantic import BaseModel
from fastapi.security import OAuth2PasswordRequestForm
from .utilities import get_sftp_bucket_name, StorageControl, fetch_institution_ids, split_csv_and_generate_signed_urls
from .config import sftp_vars, env_vars, startup_env_vars
from .authn import Token, get_current_username, check_creds, create_access_token, get_api_key
from datetime import timedelta


import os

# Set the logging
logging.basicConfig(format="%(asctime)s [%(levelname)s]: %(message)s")
logger = logging.getLogger(__name__)
logger.setLevel(logging.DEBUG)

app = FastAPI(
    servers=[
        # TODO: placeholders
        {"url": "https://stag.example.com", "description": "Staging environment"},
        {"url": "https://prod.example.com", "description": "Production environment"},
    ],
    root_path="/worker/api/v1",
)

# this uses api key to auth to backend api, but credentials to auth to this service


class PdpPullRequest(BaseModel):
    """Params for the PDP pull request."""

    placeholder: str | None = None


class PdpPullResponse(BaseModel):
    """Fields for the PDP pull response."""

    sftp_files: list[dict]
    pdp_inst_generated: list[dict]
    pdp_inst_not_found: list[str]


@app.on_event("startup")
def on_startup():
    print("Starting up app...")
    startup_env_vars()


# On shutdown, we have to cleanup the GCP database connections
@app.on_event("shutdown")
def shutdown_event():
    print("Performing shutdown tasks...")


# The following root paths don't have pre-authn.
@app.get("/")
def read_root() -> Any:
    """Returns the index.html file."""
    return FileResponse("src/worker/index.html")


@app.post("/token")
async def login_for_access_token(
    form_data: Annotated[OAuth2PasswordRequestForm, Depends()],
) -> Token:
    valid = check_creds(form_data.username, form_data.password)
    if not valid:
        raise HTTPException(
            status_code=status.HTTP_401_UNAUTHORIZED,
            detail="Incorrect username or password",
            headers={"WWW-Authenticate": "Bearer"},
        )
    access_token_expires = timedelta(
        minutes=int(env_vars["ACCESS_TOKEN_EXPIRE_MINUTES"])
    )
    access_token = create_access_token(
        data={"sub": form_data.username}, expires_delta=access_token_expires
    )
    return Token(access_token=access_token, token_type="bearer")

def sftp_helper(storage_control: StorageControl, sftp_source_filenames: list) -> list:
    """
    For each source file in sftp_source_filenames, copies the file from the SFTP
    server to GCS. The destination filename is automatically generated by prefixing
    the base name of the source file with "processed_".

    Args:
        storage_control (StorageControl): An instance with a method `copy_from_sftp_to_gcs`.
        sftp_source_filenames (list): A list of file paths on the SFTP server.
    """
    num_files = len(sftp_source_filenames)
    logger.info(f"Starting sftp_helper for {num_files} file(s).")
    all_blobs = []
    for sftp_source_filename in sftp_source_filenames:
        sftp_source_filename = sftp_source_filename["path"]
        if (
            sftp_source_filename
            == "./receive/AO1600pdp_AO1600_AR_DEIDENTIFIED_STUDYID_20250228030226.csv"
        ):
            logger.debug(f"Processing source file: {sftp_source_filename}")

            # Extract the base filename.
            base_filename = os.path.basename(sftp_source_filename)
            dest_filename = f"{base_filename}"
            logger.debug(f"Destination filename will be: {dest_filename}")

            try:
                storage_control.copy_from_sftp_to_gcs(
                    sftp_vars["SFTP_HOST"],
                    22,
                    sftp_vars["SFTP_USER"],
                    sftp_vars["SFTP_PASSWORD"],
                    sftp_source_filename,
                    get_sftp_bucket_name(env_vars["ENV"]),
                    dest_filename,
                )
                all_blobs.append(dest_filename)
                logger.info(
                    f"Successfully processed '{sftp_source_filename}' as '{dest_filename}'."
                )
                return all_blobs
            except Exception as e:
                logger.error(
                    f"Error processing '{sftp_source_filename}': {e}", exc_info=True
                )
    return all_blobs

@app.post("/execute-pdp-pull", response_model=PdpPullResponse)
async def execute_pdp_pull(
    req: PdpPullRequest,
    current_username: Annotated[str, Depends(get_current_username)],
    storage_control: Annotated[StorageControl, Depends(StorageControl)],
    api_key_enduser_tuple: str = Security(get_api_key)
) -> Any:
    """Performs the PDP pull of the file."""
    storage_control.create_bucket_if_not_exists(get_sftp_bucket_name(env_vars["ENV"]))
    files = storage_control.list_sftp_files(
        sftp_vars["SFTP_HOST"], 22, sftp_vars["SFTP_USER"], sftp_vars["SFTP_PASSWORD"]
    )
    all_blobs = sftp_helper(storage_control, files)
    signed_urls = split_csv_and_generate_signed_urls(bucket_name=get_sftp_bucket_name(env_vars["ENV"]),
                                       source_blob_name=all_blobs[0])

    valid_pdp_ids, invalid_ids = fetch_institution_ids(pdp_ids=list(signed_urls.keys()), backend_api_key=next(key for key in api_key_enduser_tuple if key is not None))

    return {
        "sftp_files": files,
        "pdp_inst_generated": [valid_pdp_ids],
        "pdp_inst_not_found": invalid_ids,
    }
